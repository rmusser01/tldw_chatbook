# Configuration for tldw-cli TUI App
# Located at: ~/.config/tldw_cli/config.toml
[general]
default_tab = "chat"  # "chat", "character", "logs", "media", "search", "ingest", "stats"
default_theme = "textual-dark"  # Default theme on startup ("textual-dark", "textual-light", or any theme name from themes.py)
palette_theme_limit = 1  # Maximum number of themes to show in command palette (0 = show all)
log_level = "INFO" # TUI Log Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
users_name = "default_user" # Default user name for the TUI

[tldw_api]
base_url = "http://127.0.0.1:8000" # Or your actual default remote endpoint
# Default auth token can be stored here, or leave empty if user must always provide
auth_token = "default-secret-key-for-single-user"

[splash_screen]
# Splash screen configuration for startup animations
enabled = true  # Enable/disable splash screen
duration = 1.5  # Duration in seconds to display splash screen
skip_on_keypress = true  # Allow users to skip with any keypress

# Card selection mode:
# - "random": Randomly selects from active_cards list (default)
# - "sequential": Cycles through active_cards in order (not yet implemented)
# - "<card_name>": Always use a specific card (e.g., "matrix", "glitch", etc.)
card_selection = "random"

show_progress = true  # Show initialization progress bar

# All available splash cards are enabled by default for variety
# To customize: Remove cards you don't want, or replace the entire list with your preferred cards
# Static cards: default, classic, compact, minimal, blueprint
# Animated cards: matrix, glitch, retro, tech_pulse, code_scroll, minimal_fade, arcade_high_score,
#                digital_rain, loading_bar, starfield, terminal_boot, glitch_reveal, ascii_morph,
#                game_of_life, scrolling_credits, spotlight_reveal, sound_bars
active_cards = [
    "default", "matrix", "glitch", "retro", "classic", "compact", "minimal",
    "tech_pulse", "code_scroll", "minimal_fade", "blueprint", "arcade_high_score",
    "digital_rain", "loading_bar", "starfield", "terminal_boot", "glitch_reveal",
    "ascii_morph", "game_of_life", "scrolling_credits", "spotlight_reveal", "sound_bars"
]

[splash_screen.effects]
# Animation effect settings
fade_in_duration = 0.3  # Fade in time in seconds
fade_out_duration = 0.2  # Fade out time in seconds
animation_speed = 1.0  # Animation playback speed multiplier

[logging]
# Log file will be placed in the same directory as the chachanotes_db_path below.
log_filename = "tldw_cli_app.log"
file_log_level = "INFO" # File Log Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_max_bytes = 10485760 # 10 MB
log_backup_count = 5

[database]
# Path to the ChaChaNotes (Character, Chat, Notes) database.
chachanotes_db_path = "~/.local/share/tldw_cli/tldw_chatbook_ChaChaNotes.db"
# Path to the Prompts database.
prompts_db_path = "~/.local/share/tldw_cli/tldw_cli_prompts.db"
# Path to the Media V2 database.
media_db_path = "~/.local/share/tldw_cli/tldw_cli_media_v2.db"
USER_DB_BASE_DIR = "~/.local/share/tldw_cli/"

[api_endpoints]
# Optional: Specify URLs for local/custom endpoints if they differ from library defaults
# These keys should match the provider names used in the app (adjust if needed)
llama_cpp = "http://localhost:8080" # Check if your API provider uses this address
koboldcpp = "http://localhost:5001/api" # Check if your API provider uses this address
Oobabooga = "http://localhost:5000/api" # Check if your API provider uses this address
Ollama = "http://localhost:11434"
vLLM = "http://localhost:8000" # Check if your API provider uses this address
Custom = "http://localhost:1234/v1"
Custom_2 = "http://localhost:5678/v1"
Custom_3 = "http://localhost:5678/v1"
Custom_4 = "http://localhost:5678/v1"
Custom_5 = "http://localhost:5678/v1"
Custom_6 = "http://localhost:5678/v1"

# Add other local URLs if needed

[providers]
# This section primarily lists providers and their *available* models for the UI dropdown.
# Actual default model/settings used for calls are defined in [api_settings.*] or [chat_defaults]/[character_defaults].
OpenAI = ["gpt-4.1-2025-04-14", "o4-mini-2025-04-16", "o3-2025-04-16", "o3-mini-2025-01-31", "o1-2024-12-17", "chatgpt-4o-latest", "gpt-4o-2024-11-20", "gpt-4o-2024-08-06", "gpt-4.1-mini-2025-04-14", "gpt-4.1-nano-2025-04-14", "gpt-4o-mini-2024-07-18", ]
Anthropic = ["claude-opus-4-20250514", "claude-sonnet-4-20250514", "claude-3-7-sonnet-20250219", "claude-3-5-sonnet-20241022", "claude-3-5-haiku-20241022", "claude-3-5-sonnet-20240620", "claude-3-haiku-20240307", "claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-2.1", "claude-2.0"]
Cohere = ["command-a-03-2025", "command-r7b-12-2024", "command-r-plus-04-2024", "command-r-plus", "command-r-08-2024", "command-r-03-2024", "command", "command-nightly", "command-light", "command-light-nightly"]
DeepSeek = ["deepseek-chat", "deepseek-reasoner"]
Groq = ["gemma2-9b-it", "mmeta-llama/Llama-Guard-4-12B", "llama-3.3-70b-versatile", "llama-3.1-8b-instant", "llama3-70b-8192", "llama3-70b-8192", "llama3-8b-8192",]
Google = ["gemini-2.5-flash-preview-05-20", "gemini-2.5-pro-preview-05-06", "gemini-2.0-flash", "gemini-2.0-flash-lite", "gemini-1.5-flash", "gemini-1.5-flash-8b", "gemini-1.5-pro", ]
HuggingFace = ["meta-llama/Meta-Llama-3.1-8B-Instruct", "meta-llama/Meta-Llama-3.1-70B-Instruct",]
MistralAI = ["open-mistral-nemo", "mistral-medium-2505", "codestral-2501", "mistral-saba-2502", "mistral-large-2411", "ministral-3b-2410", "ministral-8b-2410", "mistral-moderation-2411", "devstral-small-2505", "mistral-small-2503", ]
OpenRouter = ["openai/gpt-4o-mini", "anthropic/claude-3.7-sonnet", "google/gemini-2.0-flash-001", "google/gemini-2.5-pro-preview", "google/gemini-2.5-flash-preview", "deepseek/deepseek-chat-v3-0324:free", "deepseek/deepseek-chat-v3-0324", "openai/gpt-4.1", "anthropic/claude-sonnet-4", "deepseek/deepseek-r1:free", "anthropic/claude-3.7-sonnet:thinking", "google/gemini-flash-1.5-8b", "mistralai/mistral-nemo", "google/gemini-2.5-flash-preview-05-20", ]
# Local Providers
Llama_cpp = ["None"]
koboldcpp = ["None"]
Oobabooga = ["None"]
Ollama = ["gemma3:12b", "gemma3:4b", "gemma3:27b", "qwen3:4b", "qwen3:8b", "qwen3:14b", "qwen3:30b", "qwen3:32b", "qwen3:235b", "devstral:24b", "deepseek-r1:671b"]
vLLM = ["vllm-model-z", "vllm-model-x", "vllm-model-y", "vllm-model-a"]
Custom = ["custom-model-alpha", "custom-model-beta"]
Custom_2 = ["custom-model-gamma", "custom-model-delta"]
TabbyAPI = ["tabby-model", "tabby-model-2", "tabby-model-3"]
Aphrodite = ["aphrodite-engine", "aphrodite-engine-2"]
local-llm = ["None"] # Add if you have a specific local-llm provider entry
local_llamacpp = ["None"]
local_llamafile = ["None"]
local_ollama = ["None"]
local_vllm = ["None"]
local_onnx = ["None"]
local_transformers = ["None"]
local_mlx_lm = ["None"]

[api_settings] # Parent section for all API provider specific settings

    # --- Cloud Providers ---
    [api_settings.openai]
    api_key_env_var = "OPENAI_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "gpt-4o" # Default model for direct calls (if not overridden)
    temperature = 0.7
    top_p = 1.0 # OpenAI uses top_p (represented as maxp sometimes in UI)
    max_tokens = 4096
    timeout = 60 # seconds
    retries = 3
    retry_delay = 5 # seconds (backoff factor)
    streaming = false

    [api_settings.anthropic]
    api_key_env_var = "ANTHROPIC_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "claude-3-haiku-20240307"
    temperature = 0.7
    top_p = 1.0 # Anthropic uses top_p (represented as topp in UI)
    top_k = 0 # Anthropic specific, 0 or -1 usually disables it
    max_tokens = 4096
    timeout = 90
    retries = 3
    retry_delay = 5
    streaming = false

    [api_settings.cohere]
    api_key_env_var = "COHERE_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "command-r-plus"
    temperature = 0.3
    top_p = 0.75 # Cohere uses 'p' (represented as topp in UI)
    top_k = 0 # Cohere uses 'k'
    max_tokens = 4096 # Cohere uses max_tokens
    timeout = 90
    retries = 3
    retry_delay = 5
    streaming = false

    [api_settings.deepseek]
    api_key_env_var = "DEEPSEEK_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "deepseek-chat"
    temperature = 0.7
    top_p = 1.0 # Deepseek uses top_p (represented as topp in UI)
    max_tokens = 4096
    timeout = 60
    retries = 3
    retry_delay = 5
    streaming = false

    [api_settings.groq]
    api_key_env_var = "GROQ_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "llama3-70b-8192"
    temperature = 0.7
    top_p = 1.0 # Groq uses top_p (represented as maxp in UI)
    max_tokens = 8192
    timeout = 60
    retries = 3
    retry_delay = 5
    streaming = false

    [api_settings.google]
    api_key_env_var = "GOOGLE_API_KEY"
    api_key = "<API_KEY_HERE>"
    model = "gemini-1.5-pro-latest"
    temperature = 0.7
    top_p = 0.9 # Google uses topP (represented as topp in UI)
    top_k = 100 # Google uses topK
    max_tokens = 8192 # Google uses maxOutputTokens
    timeout = 120
    retries = 3
    retry_delay = 5
    streaming = false

    [api_settings.huggingface]
    api_key_env_var = "HUGGINGFACE_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "mistralai/Mixtral-8x7B-Instruct-v0.1"
    temperature = 0.7
    top_p = 1.0 # HF Inference API uses top_p
    top_k = 50  # HF Inference API uses top_k
    max_tokens = 4096 # HF Inf API uses max_tokens / max_new_tokens
    timeout = 60
    retries = 3
    retry_delay = 5
    streaming = false

    [api_settings.mistralai] # Matches key in [providers]
    api_key_env_var = "MISTRAL_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "mistral-large-latest"
    temperature = 0.7
    top_p = 1.0 # Mistral uses top_p (represented as topp in UI)
    max_tokens = 4096
    timeout = 60
    retries = 3
    retry_delay = 5
    streaming = false

    [api_settings.openrouter]
    api_key_env_var = "OPENROUTER_API_KEY"
    # api_key = "" # Less secure fallback - use env var instead
    model = "meta-llama/Llama-3.1-8B-Instruct"
    temperature = 0.7
    top_p = 1.0 # OpenRouter uses top_p
    top_k = 0   # OpenRouter uses top_k
    min_p = 0.0 # OpenRouter uses min_p
    max_tokens = 4096
    timeout = 120
    retries = 3
    retry_delay = 5
    streaming = false

    # --- Local Providers ---
    [api_settings.llama_cpp] # Matches key in [providers]
    api_key_env_var = "LLAMA_CPP_API_KEY" # If you set one on the server
    # api_key = ""
    api_url = "http://localhost:8080/completion" # llama.cpp /completion endpoint
    model = "" # Often not needed if server serves one model
    temperature = 0.7
    top_p = 0.95
    top_k = 40
    min_p = 0.05
    max_tokens = 4096 # llama.cpp uses n_predict
    timeout = 300
    retries = 1
    retry_delay = 2
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.oobabooga] # Matches key in [providers]
    api_key_env_var = "OOBABOOGA_API_KEY" # If API extension needs one
    api_url = "http://localhost:5000/v1/chat/completions" # Ooba OpenAI compatible endpoint
    model = "" # Model loaded in Ooba UI
    temperature = 0.7
    top_p = 0.9
    # top_k = 50 # Check Ooba endpoint docs for OpenAI compatibility params
    # min_p = 0.0
    max_tokens = 4096
    timeout = 300
    retries = 1
    retry_delay = 2
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.koboldcpp] # Matches key in [providers]
    # api_key = "" # Kobold doesn't use keys
    api_url = "http://localhost:5001/api/v1/generate" # Kobold non-streaming API
    # api_streaming_url = "http://localhost:5001/api/v1/stream" # Kobold streaming API (different format)
    model = "" # Model loaded in Kobold UI
    temperature = 0.7
    top_p = 0.9
    top_k = 50
    max_tokens = 4096 # Kobold uses max_context_length / max_length
    timeout = 300
    retries = 1
    retry_delay = 2
    streaming = false # Kobold streaming is non-standard, handle carefully
    system_prompt = "You are a helpful AI assistant"

    [api_settings.ollama]
    # No API Key usually needed
    api_url = "http://localhost:11434/v1/chat/completions" # Default Ollama OpenAI endpoint
    model = "llama3:latest"
    temperature = 0.7
    top_p = 0.9
    top_k = 40 # Ollama supports top_k via OpenAI endpoint
    # min_p = 0.05 # Ollama OpenAI endpoint doesn't support min_p directly
    max_tokens = 4096
    timeout = 300 # Longer timeout for local models
    retries = 1
    retry_delay = 2
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.vllm] # Matches key in [providers]
    api_key_env_var = "VLLM_API_KEY" # If served behind auth
    api_url = "http://localhost:8000/v1/chat/completions" # vLLM OpenAI compatible endpoint
    model = "" # Model specified when starting vLLM server
    temperature = 0.7
    top_p = 0.95
    top_k = 50
    min_p = 0.05
    max_tokens = 4096
    timeout = 300
    retries = 1
    retry_delay = 2
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.aphrodite] # Matches key in [providers]
    api_key_env_var = "APHRODITE_API_KEY" # If served behind auth
    api_url = "http://localhost:2242/v1/chat/completions" # Default Aphrodite port
    model = "aphrodite-engine" # Model loaded in Aphrodite
    temperature = 0.7
    top_p = 0.95
    top_k = 50
    min_p = 0.05
    max_tokens = 4096
    timeout = 300
    retries = 1
    retry_delay = 2
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.tabbyapi] # Matches key in [providers]
    api_key_env_var = "TABBYAPI_API_KEY"
    api_url = "http://localhost:8080/v1/chat/completions" # Check TabbyAPI docs for exact URL
    model = "tabby-model" # Model configured in TabbyAPI
    temperature = 0.7
    top_p = 0.95
    top_k = 50
    min_p = 0.05
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 3
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.custom] # Matches key in [providers]
    api_key_env_var = "CUSTOM_API_KEY"
    api_url = "http://localhost:1234/v1/chat/completions"
    model = "custom-model-alpha"
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.custom_2] # Matches key in [providers]
    api_key_env_var = "CUSTOM_2_API_KEY"
    api_url = "http://localhost:5678/v1/chat/completions"
    model = "custom-model-gamma"
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local-llm] # Matches key in [providers]
    api_url = "http://localhost:8000/v1/chat/completions"
    model = ""
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local_llamafile] # Matches key in [providers]
    api_url = "http://localhost:8001/v1/chat/completions"
    model = ""
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local_llamacpp] # Matches key in [providers]
    #api_key_env_var = "local_llamacpp_API_KEY"
    api_url = "http://localhost:8001/v1/chat/completions"
    model = "custom-model-gamma"
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local_vllm] # Matches key in [providers]
    #api_key_env_var = "local_vllm_API_KEY" # If served behind auth
    api_url = "http://localhost:8008/v1/chat/completions"
    model = "custom-model-gamma"
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local_ollama] # Matches key in [providers]
    api_key_env_var = "local_ollama_API_KEY" # If served behind auth
    api_url = "http://localhost:5678/v1/chat/completions"
    model = "custom-model-gamma"
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local_onnx] # Matches key in [providers]
    api_url = "http://localhost:8000/v1/chat/completions"
    model = ""
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local_transformers] # Matches key in [providers]
    api_url = "http://localhost:8000/v1/chat/completions"
    model = ""
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"

    [api_settings.local_mlx_lm] # Matches key in [providers]
    api_url = "http://localhost:5678/v1/chat/completions"
    model = "custom-model-gamma"
    temperature = 0.7
    top_p = 1.0
    top_k = 0
    min_p = 0.0
    max_tokens = 4096
    timeout = 120
    retries = 2
    retry_delay = 5
    streaming = false
    system_prompt = "You are a helpful AI assistant"
    # ... etc ...

[chat_defaults]
# Default settings specifically for the 'Chat' tab
provider = "DeepSeek"
model = "deepseek-chat"
system_prompt = "You are a helpful AI assistant."
temperature = 0.6
top_p = 0.95
min_p = 0.05
top_k = 50
strip_thinking_tags = true
use_enhanced_window = false  # Enable enhanced chat window with image support

# Image support settings (when use_enhanced_window = true)
[chat.images]
enabled = true
show_attach_button = true  # Show/hide the attach file button in chat
default_render_mode = "auto"  # auto, pixels, regular
max_size_mb = 10.0
auto_resize = true
resize_max_dimension = 2048
save_location = "~/Downloads"
supported_formats = [".png", ".jpg", ".jpeg", ".gif", ".webp", ".bmp"]

[chat.images.terminal_overrides]
kitty = "regular"
wezterm = "regular"
iterm2 = "regular"
default = "pixels"

[character_defaults]
# Default settings specifically for the 'Character' tab
provider = "Anthropic"
model = "claude-3-haiku-20240307" # Make sure this exists in [providers.Anthropic]
system_prompt = "You are roleplaying as a witty pirate captain."
temperature = 0.8
top_p = 0.9
min_p = 0.0 # Check if API supports this
top_k = 100 # Check if API supports this

[notes]
# Default settings for the Notes tab
sync_directory = "~/Documents/Notes"  # Default directory for notes synchronization
auto_sync_enabled = false            # Enable automatic sync on startup
sync_on_close = false               # Sync when closing the app
conflict_resolution = "newer_wins"   # Default conflict resolution: newer_wins, ask, disk_wins, db_wins
sync_direction = "bidirectional"     # Default sync direction: bidirectional, disk_to_db, db_to_disk


# ==========================================================
# Default/Template Prompts
# ==========================================================
[Prompts]
# Default prompts used by various functions. These can be overridden by user settings.
sub_question_generation_prompt = "Based on the user query and chat history, generate up to 3 sub-questions to gather more specific information. Format as a numbered list."
search_result_relevance_eval_prompt = "Evaluate the relevance of the following search result snippet to the query. Score from 1 (not relevant) to 5 (highly relevant)."
analyze_search_results_prompt = "Analyze the provided search results and synthesize a comprehensive answer to the original query."
situate_chunk_context_prompt = "You are an AI assistant. Please follow the instructions provided in the input text carefully and accurately."

[prompts.document_generation.timeline]
prompt = "Create a detailed text-based timeline based on our conversation/materials being referenced. Include key dates, events, and their relationships in chronological order."
temperature = 0.3
max_tokens = 2000

[prompts.document_generation.study_guide]
prompt = "Create a detailed and well produced study guide based on the current focus of our conversation/materials in reference. Include key concepts, definitions, learning objectives, and potential exam questions."
temperature = 0.5
max_tokens = 3000

[prompts.document_generation.briefing]
prompt = "Create a detailed and well produced executive briefing document regarding this conversation and the subject material. Include key points, actionable insights, strategic implications, and recommendations."
temperature = 0.4
max_tokens = 2500


# ==========================================================
# Embedding Configuration
# ==========================================================
[embedding_config]
default_model_id = "mxbai-embed-large-v1"
default_llm_for_contextualization = "gpt-3.5-turbo"
model_cache_dir = "~/.local/share/tldw_cli/models/embeddings"
auto_download = true
cache_size_limit_gb = 10.0

    # --- HuggingFace Models ---
    # Default model - MixedBread AI's high-quality embedding model
    [embedding_config.models.mxbai-embed-large-v1]
    provider = "huggingface"
    model_name_or_path = "mixedbread-ai/mxbai-embed-large-v1"
    dimension = 1024  # Supports Matryoshka: can use 512, 256 for speed/storage
    trust_remote_code = false
    max_length = 512
    
    [embedding_config.models.e5-small-v2]
    provider = "huggingface"
    model_name_or_path = "intfloat/e5-small-v2"
    dimension = 384
    trust_remote_code = false
    max_length = 512

    [embedding_config.models.multilingual-e5-large-instruct]
    provider = "huggingface"
    model_name_or_path = "intfloat/multilingual-e5-large-instruct"
    dimension = 1024
    trust_remote_code = false
    max_length = 512

    [embedding_config.models.e5-base-v2]
    provider = "huggingface"
    model_name_or_path = "intfloat/e5-base-v2"
    dimension = 768
    trust_remote_code = false
    max_length = 512

    [embedding_config.models.e5-large-v2]
    provider = "huggingface"
    model_name_or_path = "intfloat/e5-large-v2"
    dimension = 1024
    trust_remote_code = false
    max_length = 512

    [embedding_config.models.all-MiniLM-L6-v2]
    provider = "huggingface"
    model_name_or_path = "sentence-transformers/all-MiniLM-L6-v2"
    dimension = 384
    trust_remote_code = false
    max_length = 256

    [embedding_config.models.all-mpnet-base-v2]
    provider = "huggingface"
    model_name_or_path = "sentence-transformers/all-mpnet-base-v2"
    dimension = 768
    trust_remote_code = false
    max_length = 384

    [embedding_config.models.bge-small-en-v1.5]
    provider = "huggingface"
    model_name_or_path = "BAAI/bge-small-en-v1.5"
    dimension = 384
    trust_remote_code = false
    max_length = 512

    [embedding_config.models.bge-base-en-v1.5]
    provider = "huggingface"
    model_name_or_path = "BAAI/bge-base-en-v1.5"
    dimension = 768
    trust_remote_code = false
    max_length = 512

    [embedding_config.models.gte-small]
    provider = "huggingface"
    model_name_or_path = "thenlper/gte-small"
    dimension = 384
    trust_remote_code = false
    max_length = 512

    # --- Official OpenAI Models ---
    [embedding_config.models.openai-ada-002]
    provider = "openai"
    model_name_or_path = "text-embedding-ada-002"
    dimension = 1536
    api_key = "YOUR_OPENAI_API_KEY_OR_LEAVE_BLANK_IF_ENV_VAR_SET" # User fills this or sets ENV

    [embedding_config.models.openai-text-embedding-3-small]
    provider = "openai"
    model_name_or_path = "text-embedding-3-small" # Common model name
    dimension = 3072 # Or 256,, 1536, 2048 3072 depending on how you use it
    api_key = "YOUR_OPENAI_API_KEY_OR_LEAVE_BLANK_IF_ENV_VAR_SET"

    [embedding_config.models.openai-text-embedding-3-large]
    provider = "openai"
    model_name_or_path = "text-embedding-3-large" # Common model name
    dimension = 1536 # Or 512, 1536 depending on how you use it
    api_key = "YOUR_OPENAI_API_KEY_OR_LEAVE_BLANK_IF_ENV_VAR_SET"

    # --- Large State-of-the-Art Models (Require trust_remote_code) ---
    # Stella - Very high quality, supports multiple dimensions via Matryoshka
    # SECURITY: Pinned to specific revision to prevent malicious code injection
    [embedding_config.models.stella_en_1.5B_v5]
    provider = "huggingface"
    model_name_or_path = "NovaSearch/stella_en_1.5B_v5"
    dimension = 1024  # Supports 512, 768, 1024, 2048, 4096, 6144, 8192
    trust_remote_code = true  # Required for custom architecture
    revision = "4bbc0f1e9df5b9563d418e9b5663e98070713eb8"  # Pinned for security
    max_length = 512
    batch_size = 8
    
    # Qwen3 Embedding - 4B parameters, 32k context, 100+ languages
    [embedding_config.models.qwen3-embedding-4b]
    provider = "huggingface"
    model_name_or_path = "Qwen/Qwen3-Embedding-4B"
    dimension = 4096  # Flexible up to 4096
    trust_remote_code = true  # Required for Qwen3 architecture
    max_length = 32768  # 32k context length
    batch_size = 4  # Reduced due to model size
    
    # --- Placeholder for a Local OpenAI-Compatible Server ---
    # The user needs to edit this section for their specific local setup.
    # The 'key' (e.g., "my-local-nomic-model") is what will appear in the UI's model dropdown
    # when "Local OpenAI-Compliant Server" provider is selected.
    [embedding_config.models.my-local-nomic-model]
    provider = "openai" # CRITICAL: This tells EmbeddingFactory to use OpenAICfg
    model_name_or_path = "nomic-ai/nomic-embed-text-v1" # The actual model name the LOCAL SERVER uses/expects
    base_url = "http://localhost:8080/v1" # The base URL of THE LOCAL SERVER's OpenAI-compatible API
    dimension = 768 # CRITICAL: User MUST provide the correct dimension for this model
    # api_key can be omitted if the local server doesn't require one, or set to a dummy value.
    # api_key = "not-needed-for-local"

    # --- Another Local Example (e.g., for a Llama.cpp server with embeddings) ---
    [embedding_config.models.local-llama-cpp-embeddings]
    provider = "openai"
    model_name_or_path = "llama-2-7b-chat.Q4_K_M.gguf" # Or whatever model name the server endpoint expects
    base_url = "http://localhost:8000/v1" # Common port for Llama.cpp server's OpenAI API
    dimension = 4096 # Example dimension for Llama-2 base models
    # api_key = "sk-xxxxxxxxxxxxxxxxx" # If your Llama.cpp server is configured with an API key

# You can add more local model configurations following the pattern above.
# The key part is `provider = "openai"` and providing the correct `base_url` and `dimension`.


# ==========================================================
# RAG (Retrieval-Augmented Generation) Configuration
# ==========================================================
[rag]
# Comprehensive configuration for the RAG system

    # --- Embedding Settings ---
    [rag.embedding]
    # Default embedding model (uses model definitions from [embedding_config] section)
    model = "mxbai-embed-large-v1"    # Model ID from embedding_config.models
    device = "auto"                    # Device: "auto", "cpu", "cuda", "mps"
    cache_size = 2                     # Number of models to keep in memory
    batch_size = 32                    # Batch size for embedding generation
    max_length = 512                   # Maximum sequence length
    # API settings for OpenAI-compatible models (optional)
    # api_key = ""                     # Will use OPENAI_API_KEY env var if not set
    # base_url = ""                    # Custom endpoint for local servers
    
    # --- Retrieval Settings ---
    [rag.retriever]
    fts_top_k = 10              # Number of results from full-text search
    vector_top_k = 10           # Number of results from vector search
    hybrid_alpha = 0.5          # Weight for hybrid search (0=FTS only, 1=vector only)
    chunk_size = 512            # Size of text chunks for indexing
    chunk_overlap = 128         # Overlap between chunks
    
    # Collection names for different data types
    media_collection = "media_embeddings"
    chat_collection = "chat_embeddings"
    notes_collection = "notes_embeddings"
    character_collection = "character_embeddings"
    
    # --- Processing Settings ---
    [rag.processor]
    enable_reranking = true         # Enable result reranking
    reranker_model = "cohere"       # Reranker model: "cohere", "flashrank", or null
    reranker_top_k = 5             # Number of results to rerank
    deduplication_threshold = 0.85  # Similarity threshold for deduplication
    max_context_length = 4096      # Maximum context length for LLM
    combination_method = "weighted" # "weighted", "round_robin", "score_based"
    
    # --- Query Expansion Settings ---
    [rag.query_expansion]
    enabled = false                     # Enable query expansion/rewriting
    method = "llm"                      # Method: "llm", "local_llm", "keywords"
    max_sub_queries = 3                 # Maximum number of sub-queries to generate
    llm_provider = "openai"             # LLM provider for expansion (when method="llm")
    llm_model = "gpt-3.5-turbo"         # Model for query expansion (fast and cheap)
    local_model = "qwen2.5:0.5b"        # Local model for expansion (when method="local_llm")
    expansion_prompt_template = "default" # Prompt template or custom prompt
    combine_results = true              # Combine results from all sub-queries
    cache_expansions = true             # Cache expanded queries for reuse
    
    # --- Generation Settings ---
    [rag.generator]
    default_model = ""             # Default LLM model (empty = use chat defaults)
    default_temperature = 0.7      # Default temperature for RAG responses
    max_tokens = 1024              # Maximum tokens for RAG responses
    enable_streaming = true        # Enable streaming responses
    stream_chunk_size = 10         # Tokens per stream chunk
    
    # --- ChromaDB Settings ---
    [rag.chroma]
    persist_directory = ""         # Directory for ChromaDB (empty = auto)
    collection_prefix = "tldw_rag" # Prefix for collection names
    embedding_model = "all-MiniLM-L6-v2"  # Default embedding model
    embedding_dimension = 384      # Embedding dimension
    distance_metric = "cosine"     # "cosine", "euclidean", "ip"
    
    # --- Caching Settings ---
    [rag.cache]
    enable_cache = true            # Enable result caching
    cache_ttl = 3600              # Cache TTL in seconds (1 hour)
    max_cache_size = 1000         # Maximum cached items
    cache_embedding_results = true # Cache embedding results
    cache_search_results = true   # Cache search results
    cache_llm_responses = false   # Cache LLM responses (usually want fresh)
    
    # --- Memory Management Settings ---
    [rag.memory_management]
    max_total_size_mb = 1024.0         # Maximum total ChromaDB size (MB)
    max_collection_size_mb = 512.0     # Maximum size per collection (MB)
    max_documents_per_collection = 100000  # Maximum documents per collection
    max_age_days = 90                  # Maximum age of documents (days)
    inactive_collection_days = 30      # Days before cleaning inactive collections
    enable_automatic_cleanup = true    # Enable automatic cleanup
    cleanup_interval_hours = 24        # Hours between cleanup runs
    cleanup_batch_size = 1000         # Documents to delete per batch
    enable_lru_cache = true           # Enable ChromaDB LRU cache
    memory_limit_bytes = 2147483648   # Memory limit for ChromaDB (2GB)
    min_documents_to_keep = 100       # Minimum documents to always keep
    cleanup_confirmation_required = false  # Require confirmation for cleanup

# Legacy RAG settings (for backwards compatibility)
[rag_search]
fts_top_k = 10
vector_top_k = 10
web_vector_top_k = 10
llm_context_document_limit = 10
chat_context_limit = 10


# --- Model Capabilities Configuration ---
# ==========================================================
# Media Ingestion Settings
# ==========================================================
[media_ingestion]
# Default chunking settings for different media types during local ingestion
# These settings are used when "Default (per type)" is selected in the UI

[media_ingestion.pdf]
chunk_method = "semantic"       # Options: semantic, tokens, paragraphs, sentences, words
chunk_size = 500               # Size of each chunk
chunk_overlap = 200            # Overlap between chunks
use_adaptive_chunking = false  # Enable adaptive chunking
use_multi_level_chunking = false # Enable multi-level chunking
chunk_language = ""            # Language code for semantic chunking (e.g., "en")

[media_ingestion.ebook]
chunk_method = "ebook_chapters" # Options: ebook_chapters, tokens, paragraphs, sentences, words
chunk_size = 1000              # Size of each chunk (if not using chapters)
chunk_overlap = 200            # Overlap between chunks
use_adaptive_chunking = false  # Enable adaptive chunking
use_multi_level_chunking = false # Enable multi-level chunking
chunk_language = ""            # Language code for semantic chunking

[media_ingestion.document]
chunk_method = "sentences"      # Options: semantic, tokens, paragraphs, sentences, words
chunk_size = 1500              # Size of each chunk
chunk_overlap = 100            # Overlap between chunks
use_adaptive_chunking = false  # Enable adaptive chunking
use_multi_level_chunking = false # Enable multi-level chunking
chunk_language = ""            # Language code for semantic chunking

[media_ingestion.plaintext]
chunk_method = "paragraphs"     # Options: paragraphs, sentences, tokens, words
chunk_size = 500               # Size of each chunk
chunk_overlap = 200            # Overlap between chunks
use_adaptive_chunking = false  # Enable adaptive chunking
use_multi_level_chunking = false # Enable multi-level chunking
chunk_language = ""            # Language code for semantic chunking

[media_ingestion.web_article]
chunk_method = "paragraphs"     # Options: paragraphs, sentences, tokens, words
chunk_size = 500               # Size of each chunk
chunk_overlap = 200            # Overlap between chunks
use_adaptive_chunking = false  # Enable adaptive chunking
use_multi_level_chunking = false # Enable multi-level chunking
chunk_language = ""            # Language code for semantic chunking

# --- Model Capabilities Configuration ---
[model_capabilities]
# This section defines which models have specific capabilities like vision support.
# Users can override or extend these patterns in their config file.

# Direct model-to-capability mappings (highest priority)
[model_capabilities.models]
# OpenAI models
"gpt-4-vision-preview" = { vision = true, max_images = 1 }
"gpt-4-turbo" = { vision = true, max_images = 10 }
"gpt-4-turbo-2024-04-09" = { vision = true, max_images = 10 }
"gpt-4o" = { vision = true, max_images = 10 }
"gpt-4o-mini" = { vision = true, max_images = 10 }

# Anthropic models
"claude-3-opus-20240229" = { vision = true, max_images = 5 }
"claude-3-sonnet-20240229" = { vision = true, max_images = 5 }
"claude-3-haiku-20240307" = { vision = true, max_images = 5 }
"claude-3-5-sonnet-20240620" = { vision = true, max_images = 5 }
"claude-3-5-sonnet-20241022" = { vision = true, max_images = 5 }

# Google models
"gemini-pro-vision" = { vision = true, max_images = 1 }
"gemini-1.5-pro" = { vision = true, max_images = 10 }
"gemini-1.5-flash" = { vision = true, max_images = 10 }
"gemini-2.0-flash" = { vision = true, max_images = 10 }

# Pattern-based matching for model families (fallback if not in direct mappings)
[model_capabilities.patterns]
# OpenAI patterns
OpenAI = [
    { pattern = "^gpt-4.*vision", vision = true },
    { pattern = "^gpt-4[o0](?:-mini)?", vision = true },  # gpt-4o, gpt-40, gpt-4o-mini
    { pattern = "^gpt-4.*turbo", vision = true }
]

# Anthropic patterns
Anthropic = [
    { pattern = "^claude-3", vision = true },             # All Claude 3 models have vision
    { pattern = "^claude.*opus-4", vision = true },      # Claude Opus 4 series
    { pattern = "^claude.*sonnet-4", vision = true }     # Claude Sonnet 4 series
]

# Google patterns
Google = [
    { pattern = "gemini.*vision", vision = true },
    { pattern = "gemini-[0-9.]+-(pro|flash)", vision = true },  # Modern Gemini models
    { pattern = "gemini-2\\.", vision = true }                 # Gemini 2.x series
]

# OpenRouter patterns (uses provider/model format)
OpenRouter = [
    { pattern = "openai/gpt-4.*vision", vision = true },
    { pattern = "openai/gpt-4[o0]", vision = true },
    { pattern = "anthropic/claude-3", vision = true },
    { pattern = "google/gemini.*vision", vision = true },
    { pattern = "google/gemini-[0-9.]+-(pro|flash)", vision = true }
]

# Default behavior for unknown models
[model_capabilities.defaults]
unknown_models_vision = false  # Whether to assume unknown models have vision capabilities
log_unknown_models = true      # Whether to log when an unknown model is queried

# --- Sections below are placeholders based on config.txt, integrate as needed ---
# [tts_settings]
# default_provider = "kokoro"
# ...

# [search_settings]
# default_provider = "google"
# ...