
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "tldw_chatbook"
version = "0.1.7.2"
authors = [
  { name="Robert Musser", email="contact@rmusser.net" },
]
description = "A Textual TUI for chatting with LLMs, and interacting with the tldw server."
readme = "README.md"
requires-python = ">=3.11"
license = {text = "AGPL-3.0-or-later"}
keywords = ["tui", "cli", "llm", "textual", "ai", "chat", "rag", "embeddings", "terminal"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: End Users/Desktop",
    "Intended Audience :: Science/Research",
    "Topic :: Utilities",
    "Topic :: Terminals",
    "Topic :: Communications :: Chat",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Environment :: Console",
    "Operating System :: OS Independent",
    "Natural Language :: English",
    "Typing :: Typed",
]

# --- Dependencies ---
dependencies = [
    "chardet",
    "httpx",
    "loguru",
    "textual>=3.3.0",
    "requests",
    "rich",
    "rich-pixels>=3.0.0",
    "pillow",
    "PyYAML",
    "pydantic",
    "psutil",
    "toml",
    "tomli; python_version < '3.11'",
    "pyperclip",
    "emoji",
    "jinja2",
    "pycryptodomex",
    "keyring",
    "markdownify",
    "aiofiles",
    "textual-image",
    "anytree",
]

# --- Optional Dependencies ---
[project.optional-dependencies]
coding_map = [
    "grep_ast",
    "pygments",
    "tqdm",

]
chunker = [
    "langdetect",
    "nltk",
    "scikit-learn",
    "fugashi",
    "transformers",
    "tqdm",
    "jieba",
]
embeddings_rag = [
    "chromadb",
    "torch",
    "numpy",
    "pydantic",
    "transformers",
    "chromadb>=0.4.0",
    "sentence-transformers",
    "opentelemetry-api",
    "opentelemetry-sdk",
]
websearch = [
    "lxml",
    "beautifulsoup4",
    "pandas",
    "playwright",
    #"playwright_stealth",
    "trafilatura",
    "langdetect",
    "nltk",
    "scikit-learn",
    "aiohttp",
    "defusedxml",
    "tqdm",
]
local_vllm = ["vllm"]
local_mlx = ["mlx-lm"]
# Individual transcription providers
transcription_faster_whisper = [
    "faster-whisper",  # CPU/CUDA optimized Whisper
]
transcription_lightning_whisper = [
    "lightning-whisper-mlx; sys_platform == 'darwin'",  # Fast Whisper for Apple Silicon
]
transcription_parakeet = [
    "parakeet-mlx; sys_platform == 'darwin'",  # Real-time ASR for Apple Silicon
]
# Legacy alias for backward compatibility
mlx_whisper = [
    "lightning-whisper-mlx; sys_platform == 'darwin'",  # Fast Whisper for Apple Silicon
    "parakeet-mlx; sys_platform == 'darwin'",  # Real-time ASR for Apple Silicon
]
local_transformers = ["transformers"]
mcp = [
    "mcp[cli]>=1.0.0",
]
pdf = [
    "pymupdf",
    "pymupdf4llm",
    "docling",
]
ebook = [
    "ebooklib",
    "beautifulsoup4",
    "lxml",
    "html2text",
    "markdownify",
    "defusedxml",
]
audio = [
    "soundfile",
    "scipy",
    "yt-dlp",
    "numpy",
    "pydub",
    "faster-whisper",  # Default transcription provider
    # Note: For Apple Silicon optimized transcription, also install:
    # pip install -e ".[audio,transcription_lightning_whisper]" or
    # pip install -e ".[audio,transcription_parakeet]"
]
video = [
    "soundfile",
    "scipy",
    "yt-dlp",
    "numpy",
    "faster-whisper",  # Default transcription provider
    # Note: For Apple Silicon optimized transcription, also install:
    # pip install -e ".[video,transcription_lightning_whisper]" or
    # pip install -e ".[video,transcription_parakeet]"
]
media_processing = [
    "soundfile",
    "scipy",
    "yt-dlp",
    "numpy",
    "faster-whisper",  # Default transcription provider
    # Note: For Apple Silicon optimized transcription, also install:
    # pip install -e ".[media_processing,transcription_lightning_whisper]" or
    # pip install -e ".[media_processing,transcription_parakeet]"
]
chatterbox = [
    "chatterbox-tts",
    "torchaudio",
    "torch",
    "faster-whisper",  # For validation of generated audio
]
mindmap = [
    # anytree is already in base dependencies
    # No additional dependencies needed for basic functionality
]
local_tts = [
    "kokoro-onnx",
    "scipy",
    "numpy",
    "nltk",
    "pyaudio",
    "pydub",
    "transformers",  # For tokenization
    "torch",  # For PyTorch-based models
    "onnxruntime",  # For ONNX models
    "av",  # pyav for audio format conversion
]
higgs_tts = [
    # IMPORTANT: Higgs Audio requires manual installation BEFORE using this extra!
    # The boson-multimodal package cannot be installed via pip and must be installed from GitHub:
    #
    # Step 1: Clone and install Higgs Audio (REQUIRED):
    #   git clone https://github.com/boson-ai/higgs-audio.git
    #   cd higgs-audio
    #   pip install -r requirements.txt
    #   pip install -e .
    #   cd ..
    #
    # Step 2: Install tldw_chatbook with Higgs support:
    #   pip install -e ".[higgs_tts]"
    #
    # The following are supporting dependencies for Higgs Audio:
    "torch",  # PyTorch for model inference
    "torchaudio",  # For audio processing and voice cloning
    "numpy",  # For audio data manipulation
    "scipy",  # For audio signal processing
    "librosa",  # For advanced audio feature extraction
    "soundfile",  # For audio file I/O
    "transformers",  # For tokenization and text processing
]
nemo = [
    "nemo-toolkit[asr]>=1.20.0",  # For NVIDIA Parakeet and Canary models
    "torch",  # Required by NeMo
    "torchaudio",  # Required for audio processing
]
speech_recording = [
    "pyaudio>=0.2.14",  # Primary audio backend
    "sounddevice>=0.4.6",  # Fallback audio backend
    "webrtcvad>=2.0.10",  # Voice activity detection
]
ocr_docext = [
    "docext",  # NanoNets document extraction toolkit
    "gradio_client",  # For API-based usage
    "transformers>=4.30.0",  # For direct model usage (optional)
    "torch",  # For direct model usage (optional)
    "openai",  # For OpenAI-compatible API usage
    "Pillow",  # For image handling
]
dev = [
    "pytest",
    "pytest-timeout",  # For handling hanging tests
    "textual-dev", # For Textual development tools
    "hypothesis",
    "pytest-asyncio",
    "build",  # For building distributions
    "twine",  # For uploading to PyPI
    "wheel",  # For building wheel distributions
    "prometheus_client",  # For metrics collection during development
]
debugging = [
    "prometheus_client",  # For metrics collection and monitoring
    "opentelemetry-api",  # For advanced telemetry
    "opentelemetry-sdk",  # OpenTelemetry SDK
    "opentelemetry-exporter-prometheus",  # Export metrics to Prometheus
    "opentelemetry-instrumentation-system-metrics",  # System metrics collection
]
diarization = [
    "torch",  # PyTorch for neural models
    "torchaudio",  # Audio processing utilities
    "speechbrain",  # Speaker embedding models
    "scikit-learn",  # For clustering algorithms (already in chunker deps)
    "numpy",  # Numerical operations (already in base deps)
]
subscriptions = [
    "markdown",  # For HTML generation from markdown
    "schedule",  # For scheduled tasks
    "feedparser",  # For RSS feed parsing
    "cryptography", # Security.py
    "defusedxml",  # For safe XML parsing
    "tqdm",
]
web = [
    "textual-serve",  # Web server for running Textual apps in browser
]


[project.urls]
"Homepage" = "https://github.com/rmusser01/tldw_chatbook"
"Bug Tracker" = "https://github.com/rmusser01/tldw_chatbook/issues"
"Documentation" = "https://github.com/rmusser01/tldw_chatbook#readme"
"Source Code" = "https://github.com/rmusser01/tldw_chatbook"
"Changelog" = "https://github.com/rmusser01/tldw_chatbook/blob/main/CHANGELOG.md"

# This creates a command-line script called 'tldw-chatbook'
[project.scripts]
# Points to the main function in your tldw_chatbook package's app module.
# Structure: project_root/tldw_chatbook/app.py, function main_cli_runner
tldw-cli = "tldw_chatbook.app:main_cli_runner"
# Direct web server entry point
tldw-serve = "tldw_chatbook.Web_Server.serve:main"

[tool.setuptools.packages.find]
where = ["."]  # Look for packages in the current directory (where pyproject.toml is)
# Include the 'tldw_chatbook' package and its submodules
include = ["tldw_chatbook*"]
exclude = ["Tests*", ".venv*"]

[tool.setuptools.package-data]
# Include all .tcss files in css directory and subdirectories
"tldw_chatbook.css" = ["*.tcss", "**/*.tcss"]
# Include theme files
"tldw_chatbook.css.Themes" = ["*.tcss"]
# Include JSON template files and guides
"tldw_chatbook.Config_Files" = ["*.json", "*.md"]
# Include CSS files in subdirectories
"tldw_chatbook.css.core" = ["*.tcss"]
"tldw_chatbook.css.features" = ["*.tcss"]
"tldw_chatbook.css.layout" = ["*.tcss"]
# If you have other assets inside your tldw_chatbook package (e.g., tldw_chatbook/assets/), add them here:
# "tldw_chatbook.assets" = ["*"]

[tool.pytest.ini_options]
testpaths = ["Tests"]
# Exclude server tests that don't belong in the TUI
addopts = "--ignore=STests"
# Default timeout for all tests (in seconds)
timeout = 300
# Timeout method: thread (default) or signal
timeout_method = "thread"
markers = [
    "unit: marks tests as unit tests",
    "integration: marks tests as integration tests",
    "asyncio: marks tests as async",
    "timeout: marks tests with custom timeout values",
    "optional: marks tests that require optional dependencies",
    "performance: marks tests as performance tests",
    "property: marks tests as property-based tests",
    "slow: marks tests that take a long time to run",
]

