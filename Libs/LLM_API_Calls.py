# LLM_API_Calls.py
#########################################
# General LLM API Calling Library
# This library is used to perform API Calls against commercial LLM endpoints.
#
####
####################
# Function List
#
# 1. extract_text_from_segments(segments: List[Dict]) -> str
# 2. chat_with_openai(api_key, file_path, custom_prompt_arg, streaming=None)
# 3. chat_with_anthropic(api_key, file_path, model, custom_prompt_arg, max_retries=3, retry_delay=5, streaming=None)
# 4. chat_with_cohere(api_key, file_path, model, custom_prompt_arg, streaming=None)
# 5. chat_with_groq(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None):
# 6. chat_with_openrouter(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 7. chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 8. chat_with_deepseek(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
#
#
####################
#
# Import necessary libraries
import json
import os
import time
from typing import List
#
# Import 3rd-Party Libraries
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
#
# Import Local libraries
from tldw_cli.Libs.Utils import load_and_log_configs, logging
#
#######################################################################################################################
# Function Definitions
#

# FIXME: Update to include full arguments

def extract_text_from_segments(segments):
    logging.debug(f"Segments received: {segments}")
    logging.debug(f"Type of segments: {type(segments)}")

    text = ""

    if isinstance(segments, list):
        for segment in segments:
            logging.debug(f"Current segment: {segment}")
            logging.debug(f"Type of segment: {type(segment)}")
            if 'Text' in segment:
                text += segment['Text'] + " "
            else:
                logging.warning(f"Skipping segment due to missing 'Text' key: {segment}")
    else:
        logging.warning(f"Unexpected type of 'segments': {type(segments)}")

    return text.strip()


def get_openai_embeddings(input_data: str, model: str) -> List[float]:
    """
    Get embeddings for the input text from OpenAI API.

    Args:
        input_data (str): The input text to get embeddings for.
        model (str): The model to use for generating embeddings.

    Returns:
        List[float]: The embeddings generated by the API.
    """
    loaded_config_data = load_and_log_configs()
    api_key = loaded_config_data['openai_api']['api_key']

    if not api_key:
        logging.error("OpenAI: API key not found or is empty")
        raise ValueError("OpenAI: API Key Not Provided/Found in Config file or is empty")

    logging.debug(f"OpenAI: Using API Key: {api_key[:5]}...{api_key[-5:]}")
    logging.debug(f"OpenAI: Raw input data (first 500 chars): {str(input_data)[:500]}...")
    logging.debug(f"OpenAI: Using model: {model}")

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }

    request_data = {
        "input": input_data,
        "model": model,
    }

    try:
        logging.debug("OpenAI: Posting request to embeddings API")
        response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=request_data)
        logging.debug(f"Full API response data: {response}")
        if response.status_code == 200:
            response_data = response.json()
            if 'data' in response_data and len(response_data['data']) > 0:
                embedding = response_data['data'][0]['embedding']
                logging.debug("OpenAI: Embeddings retrieved successfully")
                return embedding
            else:
                logging.warning("OpenAI: Embedding data not found in the response")
                raise ValueError("OpenAI: Embedding data not available in the response")
        else:
            logging.error(f"OpenAI: Embeddings request failed with status code {response.status_code}")
            logging.error(f"OpenAI: Error response: {response.text}")
            raise ValueError(f"OpenAI: Failed to retrieve embeddings. Status code: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"OpenAI: Error making API request: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI: Error making API request: {str(e)}")
    except Exception as e:
        logging.error(f"OpenAI: Unexpected error: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI: Unexpected error occurred: {str(e)}")


def chat_with_openai(api_key, input_data, custom_prompt_arg, temp, system_message, streaming, maxp, model):
    loaded_config_data = load_and_log_configs()
    openai_api_key = api_key
    # https://platform.openai.com/docs/api-reference/invite
    try:
        # API key validation
        if not openai_api_key:
            logging.info("OpenAI: API key not provided as parameter")
            logging.info("OpenAI: Attempting to use API key from config file")
            openai_api_key = loaded_config_data['openai_api']['api_key']

        if not openai_api_key or openai_api_key == "":
            logging.error("OpenAI: API key not found or is empty")
            return "OpenAI: API Key Not Provided/Found in Config file or is empty"

        logging.debug(f"OpenAI: Using API Key: {openai_api_key[:5]}...{openai_api_key[-5:]}")

        # Streaming mode
        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data['openai_api']['streaming']
        if streaming:
            logging.debug("OpenAI: Streaming mode enabled")
        else:
            logging.debug("OpenAI: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        # Set Top-P
        if maxp is None:
            maxp = loaded_config_data['openai_api']['top_p']
            maxp = float(maxp)

        # Set model
        openai_model = model
        if openai_model is None:
            openai_model = loaded_config_data['openai_api']['model'] or "gpt-4o"
            logging.debug(f"OpenAI: Using model: {openai_model}")


        logging.debug(f"OpenAI: Custom prompt: {custom_prompt_arg}")

        headers = {
            'Authorization': f'Bearer {openai_api_key}',
            'Content-Type': 'application/json'
        }

        logging.debug(
            f"OpenAI API Key: {openai_api_key[:5]}...{openai_api_key[-5:] if openai_api_key else None}")
        logging.debug("openai: Preparing data + prompt for submittal")
        openai_prompt = f"{input_data} \n\n\n\n{custom_prompt_arg}"

        # Set Temperature
        if temp is None:
            temp = loaded_config_data['openai_api']['temperature']
            temp = float(temp)

        # Set System message
        if system_message is None:
            system_message = "You are a helpful AI assistant who does whatever the user requests."

        # Set Max Tokens
        max_tokens = loaded_config_data['openai_api']['max_tokens']
        max_tokens = int(max_tokens)
        logging.debug(f"OpenAI: Using max_tokens: {max_tokens}")

        data = {
            "model": openai_model,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": openai_prompt}
            ],
            "max_completion_tokens": max_tokens,
            "temperature": temp,
            "stream": streaming,
            "top_p": maxp
        }
        if streaming:
            logging.debug("OpenAI: Posting request (streaming")
            response = requests.post(
                'https://api.openai.com/v1/chat/completions',
                headers=headers,
                json=data,
                stream=True
            )
            logging.debug(f"OpenAI: Response text: {response.text}")
            response.raise_for_status()

            def stream_generator():
                collected_messages = ""
                for line in response.iter_lines():
                    line = line.decode("utf-8").strip()

                    if line == "":
                        continue

                    if line.startswith("data: "):
                        data_str = line[len("data: "):]
                        if data_str == "[DONE]":
                            break
                        try:
                            data_json = json.loads(data_str)
                            chunk = data_json["choices"][0]["delta"].get("content", "")
                            collected_messages += chunk
                            yield chunk
                        except json.JSONDecodeError:
                            logging.error(f"OpenAI: Error decoding JSON from line: {line}")
                            continue

            return stream_generator()
        else:
            logging.debug("OpenAI: Posting request (non-streaming")

            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['openai_api']['api_retries']
            retry_delay = loaded_config_data['openai_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            response = session.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data,
                                     timeout=30)
            logging.debug(f"Full API response data: {response}")
            if response.status_code == 200:
                response_data = response.json()
                logging.debug(response_data)
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    chat_response = response_data['choices'][0]['message']['content'].strip()
                    logging.debug("openai: Chat Sent successfully")
                    logging.debug(f"openai: Chat response: {chat_response}")
                    return chat_response
                else:
                    logging.warning("openai: Chat response not found in the response data")
                    return "openai: Chat not available"
            else:
                logging.error(f"OpenAI: Chat request failed with status code {response.status_code}")
                logging.error(f"OpenAI: Error response: {response.text}")
                return f"OpenAI: Failed to process chat response. Status code: {response.status_code}"
    except json.JSONDecodeError as e:
        logging.error(f"OpenAI: Error decoding JSON: {str(e)}", exc_info=True)
        return f"OpenAI: Error decoding JSON input: {str(e)}"
    except requests.RequestException as e:
        logging.error(f"OpenAI: Error making API request: {str(e)}", exc_info=True)
        return f"OpenAI: Error making API request: {str(e)}"
    except Exception as e:
        logging.error(f"OpenAI: Unexpected error: {str(e)}", exc_info=True)
        return f"OpenAI: Unexpected error occurred: {str(e)}"


def chat_with_anthropic(api_key, input_data, model, custom_prompt_arg, max_retries=3, retry_delay=5,
                        system_prompt=None, temp=None, streaming=False, topp=None, topk=None):
    try:
        # https://docs.anthropic.com/en/api/messages
        loaded_config_data = load_and_log_configs()

        # Check if config was loaded successfully
        if loaded_config_data is None:
            logging.error("Anthropic Chat: Failed to load configuration data.")
            return "Anthropic Chat: Failed to load configuration data."

        # Initialize the API key
        anthropic_api_key = api_key

        # API key validation
        if not api_key:
            logging.info("Anthropic Chat: API key not provided as parameter")
            logging.info("Anthropic Chat: Attempting to use API key from config file")
            # Ensure 'api_keys' and 'anthropic' keys exist
            try:
                anthropic_api_key = loaded_config_data['anthropic_api']['api_key']
                logging.debug(f"Anthropic: Loaded API Key from config: {anthropic_api_key[:5]}...{anthropic_api_key[-5:]}")
            except (KeyError, TypeError) as e:
                logging.error(f"Anthropic Chat: Error accessing API key from config: {str(e)}")
                return "Anthropic Chat: API Key Not Provided/Found in Config file or is empty"

        if not anthropic_api_key or anthropic_api_key == "":
            logging.error("Anthropic Chat: API key not found or is empty")
            return "Anthropic Chat: API Key Not Provided/Found in Config file or is empty"

        if anthropic_api_key:
            logging.debug(f"Anthropic Chat: Using API Key: {anthropic_api_key[:5]}...{anthropic_api_key[-5:]}")
        else:
            logging.debug(f"Anthropic Chat: Using API Key: {api_key[:5]}...{api_key[-5:]}")

        if system_prompt is not None:
            logging.debug("Anthropic Chat: Using provided system prompt")
            pass
        else:
            system_prompt = "You are a helpful assistant"
            logging.debug("Anthropic Chat: Using default system prompt")

        logging.debug(f"Anthropic Chat: Loaded data: {input_data}")
        logging.debug(f"Anthropic Chat: Type of data: {type(input_data)}")

        # Retrieve the model from config if not provided
        if model is None:
            try:
                anthropic_model = loaded_config_data['anthropic_api']['model']
                logging.debug(f"Anthropic Chat: Loaded model from config: {anthropic_model}")
            except (KeyError, TypeError) as e:
                logging.error(f"Anthropic Chat: Error accessing model from config: {str(e)}")
                return "Anthropic Chat: Model configuration not found."
        else:
            anthropic_model = model
            logging.debug(f"Anthropic Chat: Using provided model: {anthropic_model}")

        # Temperature
        if not isinstance(temp, float):
            temp = loaded_config_data['anthropic_api']['temperature']
            temp = float(temp)
            logging.debug(f"Anthropic Chat: Using temperature from config.txt: {temp}")
        elif isinstance(temp, float):
            temp = float(temp)
            logging.debug(f"Anthropic Chat: Using provided temperature: {temp}")
        else:
            logging.error("Anthropic Chat: Invalid value for temperature")
            return "Anthropic Chat: Invalid value for temperature"

        # Top-K
        # if not isinstance(topk, int):
        #     topk = loaded_config_data['anthropic_api']['top_k']
        #     top_k = int(topk)
        #     logging.debug(f"Anthropic Chat: Using Top-K from config.txt: {topk}")
        # elif isinstance(topk, int):
        #     top_k = topk
        #     logging.debug(f"Anthropic Chat: Using provided Top-K: {topk}")
        # else:
        #     logging.error("Anthropic Chat: Invalid value for topk")
        #     return "Anthropic Chat: Invalid value for topk"

        # Top-P
        if topp is None:
            topp = loaded_config_data['anthropic_api']['top_p']
            top_p = float(topp)
            logging.debug(f"Anthropic Chat: Using max_p from config.txt: {top_p}")
        else:
            top_p = 1.0
            logging.debug(f"Anthropic Chat: Using default maxp: {top_p}")

        # Set max tokens
        max_tokens = loaded_config_data['anthropic_api']['max_tokens']
        max_tokens = int(max_tokens)

        headers = {
            'x-api-key': anthropic_api_key,
            'anthropic-version': '2023-06-01',
            'Content-Type': 'application/json'
        }

        anthropic_user_prompt = custom_prompt_arg if custom_prompt_arg else ""
        logging.debug(f"Anthropic: User Prompt is '{anthropic_user_prompt}'")
        user_message = {
            "role": "user",
            "content": f"{input_data} \n\n\n\n{anthropic_user_prompt}"
        }

        # FIXME - add topk only if it's not None
        data = {
            "model": anthropic_model,
            "max_tokens": max_tokens,
            "messages": [user_message],
            "stop_sequences": ["\n\nHuman:"],
            "temperature": temp,
            #"top_k": top_k,
            "top_p": top_p,
            "metadata": {
                "user_id": "example_user_id",
            },
            "stream": streaming,
            "system": system_prompt
        }

        for attempt in range(max_retries):
            try:
                # Create a session
                session = requests.Session()

                # Load config values
                retry_count = loaded_config_data['anthropic_api']['api_retries']
                retry_delay = loaded_config_data['anthropic_api']['api_retry_delay']

                # Configure the retry strategy
                retry_strategy = Retry(
                    total=retry_count,  # Total number of retries
                    backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                    status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
                )

                # Create the adapter
                adapter = HTTPAdapter(max_retries=retry_strategy)

                # Mount adapters for both HTTP and HTTPS
                session.mount("http://", adapter)
                session.mount("https://", adapter)

                response = session.post(
                    'https://api.anthropic.com/v1/messages',
                    headers=headers,
                    json=data
                )

                # Check if the status code indicates success
                if response.status_code == 200:
                    if streaming:
                        # Handle streaming response
                        def stream_generator():
                            collected_text = ""
                            event_type = None
                            for line in response.iter_lines():
                                line = line.decode('utf-8').strip()
                                if line == '':
                                    continue
                                if line.startswith('event:'):
                                    event_type = line[len('event:'):].strip()
                                elif line.startswith('data:'):
                                    data_str = line[len('data:'):].strip()
                                    if data_str == '[DONE]':
                                        break
                                    try:
                                        data_json = json.loads(data_str)
                                        if event_type == 'content_block_delta' and data_json.get(
                                                'type') == 'content_block_delta':
                                            delta = data_json.get('delta', {})
                                            text_delta = delta.get('text', '')
                                            collected_text += text_delta
                                            yield text_delta
                                    except json.JSONDecodeError:
                                        logging.error(f"Anthropic: Error decoding JSON from line: {line}")
                                        continue
                            # Optionally, return the full collected text at the end
                            # yield collected_text

                        return stream_generator()
                    else:
                        # Non-streaming response
                        logging.debug("Anthropic: Post submittal successful")
                        response_data = response.json()
                        try:
                            # Extract the assistant's reply from the 'content' field
                            content_blocks = response_data.get('content', [])
                            summary = ''
                            for block in content_blocks:
                                if block.get('type') == 'text':
                                    summary += block.get('text', '')
                            summary = summary.strip()
                            logging.debug("Anthropic: Summarization successful")
                            logging.debug(f"Anthropic: Summary (first 500 chars): {summary[:500]}...")
                            return summary
                        except Exception as e:
                            logging.debug("Anthropic: Unexpected data in response")
                            logging.error(f"Unexpected response format from Anthropic API: {response.text}")
                            return None
                elif response.status_code == 500:  # Handle internal server error specifically
                    logging.debug("Anthropic: Internal server error")
                    logging.error("Internal server error from API. Retrying may be necessary.")
                    time.sleep(retry_delay)
                else:
                    logging.debug(
                        f"Anthropic: Failed to summarize, status code {response.status_code}: {response.text}")
                    logging.error(f"Failed to process summary, status code {response.status_code}: {response.text}")
                    return None

            except requests.RequestException as e:
                logging.error(f"Anthropic: Network error during attempt {attempt + 1}/{max_retries}: {str(e)}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                else:
                    return f"Anthropic: Network error: {str(e)}"
    except FileNotFoundError as e:
        logging.error(f"Anthropic: File not found: {input_data}")
        return f"Anthropic: File not found: {input_data}"
    except json.JSONDecodeError as e:
        logging.error(f"Anthropic: Invalid JSON format in file: {input_data}")
        return f"Anthropic: Invalid JSON format in file: {input_data}"
    except Exception as e:
        logging.error(f"Anthropic: Error in processing: {str(e)}")
        return f"Anthropic: Error occurred while processing summary with Anthropic: {str(e)}"


# Summarize with Cohere
def chat_with_cohere(api_key, input_data, model=None, custom_prompt_arg=None, system_prompt=None, temp=None, streaming=False, topp=None, topk=None):
    loaded_config_data = load_and_log_configs()
    cohere_api_key = None
    # https://docs.cohere.com/v2/reference/chat-stream
    try:
        # API key validation
        if api_key:
            logging.info(f"Cohere Chat: API Key from parameter: {api_key[:3]}...{api_key[-3:]}")
            cohere_api_key = api_key
        else:
            logging.info("Cohere Chat: API key not provided as parameter")
            logging.info("Cohere Chat: Attempting to use API key from config file")
            logging.debug(f"Cohere Chat: Cohere API Key from config: {loaded_config_data['cohere_api']['api_key']}")
            cohere_api_key = loaded_config_data['cohere_api']['api_key']
            if cohere_api_key:
                logging.debug(f"Cohere Chat: Cohere API Key from config: {cohere_api_key[:3]}...{cohere_api_key[-3:]}")
            else:
                logging.error("Cohere Chat: API key not found or is empty")
                return "Cohere Chat: API Key Not Provided/Found in Config file or is empty"

        logging.debug(f"Cohere Chat: Loaded data: {input_data}")
        logging.debug(f"Cohere Chat: Type of data: {type(input_data)}")

        # Streaming mode
        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data.get('cohere_api', {}).get('streaming', False)
            logging.debug("Cohere: Streaming mode enabled")
        else:
            logging.debug("Cohere: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        # Top-K
        if not isinstance(topk, int):
            top_k = loaded_config_data['cohere_api']['top_k']
            top_k = int(top_k)
            logging.debug(f"Cohere Chat: Using top_k from config: {top_k}")

        # Max-P
        if not isinstance(topp, float):
            topp = loaded_config_data['cohere_api']['max_p']
            topp = float(topp)
            logging.debug(f"Cohere Chat: Using max_p from config: {topp}")

        # Temperature
        if not isinstance(temp, float):
            temp = loaded_config_data['cohere_api']['temperature']
            temp = float(temp)
            logging.debug(f"Cohere Chat: Using temperature from config: {temp}")

        # Model
        if not isinstance(model, str):
            model = loaded_config_data['cohere_api']['model']
            logging.debug(f"Cohere Chat: Using model from config: {model}")

        headers = {
            'accept': 'application/json',
            'content-type': 'application/json',
            'Authorization': f'Bearer {cohere_api_key}'
        }

        # Ensure system_prompt is set
        if not isinstance(system_prompt, str):
            system_prompt = "You are a helpful assistant"
        logging.debug(f"Cohere Chat: System Prompt being sent is: '{system_prompt}'")

        cohere_prompt = input_data
        if custom_prompt_arg:
            cohere_prompt += f"\n\n{custom_prompt_arg}"
        logging.debug(f"Cohere Chat: User Prompt being sent is: '{cohere_prompt}'")

        data = {
            "model" : model,
            "temperature": temp,
            "messages": [
                {
                    "role": "system",
                    "content":  system_prompt
                },
                {
                    "role": "user",
                    "content": cohere_prompt,
                }
            ],
            "k": top_k,
            "p": topp,
        }
        logging.debug(f"Cohere Chat: Request data: {json.dumps(data, indent=2)}")

        logging.debug("cohere chat: Submitting request to API endpoint")
        logging.info("cohere chat: Submitting request to API endpoint")

        if streaming:
            logging.debug("Cohere: Submitting streaming request to API endpoint")
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['cohere_api']['api_retries']
            retry_delay = loaded_config_data['cohere_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            response = session.post(
                'https://api.cohere.com/v2/chat',
                headers=headers,
                json=data,
                stream=True  # Enable response streaming
            )
            logging.debug(f"Cohere Chat: Raw API response: {response.text}")
            response.raise_for_status()

            def stream_generator():
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8').strip()
                        try:
                            data_json = json.loads(decoded_line)
                            if 'response' in data_json:
                                chunk = data_json['response']
                                yield chunk
                            elif 'token' in data_json:
                                # For token-based streaming (if applicable)
                                chunk = data_json['token']
                                yield chunk
                            elif 'text' in data_json:
                                # For text-based streaming
                                chunk = data_json['text']
                                yield chunk
                            elif 'message' in data_json and 'content' in data_json['message']:
                                # content is usually an array of blocks [{ "type": "text", "text": ... }, ...]
                                for block in data_json['message']['content']:
                                    if block.get('type') == 'text':
                                        chunk = block.get('text', '')
                                        if chunk:
                                            yield chunk
                            else:
                                logging.debug(f"Cohere: Unhandled streaming data: {data_json}")
                        except json.JSONDecodeError:
                            logging.error(f"Cohere: Error decoding JSON from line: {decoded_line}")
                            continue

            return stream_generator()
        else:
            try:
                # Create a session
                session = requests.Session()

                # Load config values
                retry_count = loaded_config_data['cohere_api']['api_retries']
                retry_delay = loaded_config_data['cohere_api']['api_retry_delay']

                # Configure the retry strategy
                retry_strategy = Retry(
                    total=retry_count,  # Total number of retries
                    backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                    status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
                )

                # Create the adapter
                adapter = HTTPAdapter(max_retries=retry_strategy)

                # Mount adapters for both HTTP and HTTPS
                session.mount("http://", adapter)
                session.mount("https://", adapter)
                response = requests.post('https://api.cohere.com/v2/chat', headers=headers, json=data)
                logging.debug(f"Cohere Chat: Raw API response: {response.text}")
            except requests.RequestException as e:
                logging.error(f"Cohere Chat: Error making API request: {str(e)}")
                return f"Cohere Chat: Error making API request: {str(e)}"

            if response.status_code == 200:
                try:
                    response_data = response.json()
                except json.JSONDecodeError:
                    logging.error("Cohere Chat: Failed to decode JSON response")
                    return "Cohere Chat: Failed to decode JSON response"

                if response_data is None:
                    logging.error("Cohere Chat: No response data received.")
                    return "Cohere Chat: No response data received."

                logging.debug(f"cohere chat: Full API response data: {json.dumps(response_data, indent=2)}")

                if 'message' in response_data and 'content' in response_data['message']:
                    content = response_data['message']['content']
                    if isinstance(content, list) and len(content) > 0:
                        # Extract text from the first content block
                        text = content[0].get('text', '').strip()
                        if text:
                            logging.debug("Cohere Chat: Chat request successful")
                            print("Cohere Chat request processed successfully.")
                            return text
                        else:
                            logging.error("Cohere Chat: 'text' field is empty in response content.")
                            return "Cohere Chat: 'text' field is empty in response content."
                    else:
                        logging.error("Cohere Chat: 'content' field is not a list or is empty.")
                        return "Cohere Chat: 'content' field is not a list or is empty."
                else:
                    logging.error("Cohere Chat: 'message' or 'content' field not found in API response.")
                    return "Cohere Chat: 'message' or 'content' field not found in API response."

            elif response.status_code == 401:
                error_message = "Cohere Chat: Unauthorized - Invalid API key"
                logging.warning(error_message)
                print(error_message)
                return error_message

            else:
                logging.error(f"Cohere Chat: API request failed with status code {response.status_code}: {response.text}")
                print(f"Cohere Chat: Failed to process chat response, status code {response.status_code}: {response.text}")
                return f"Cohere Chat: API request failed: {response.text}"

    except Exception as e:
        logging.error(f"Cohere Chat: Error in processing: {str(e)}", exc_info=True)
        return f"Cohere Chat: Error occurred while processing chat request with Cohere: {str(e)}"


# https://console.groq.com/docs/quickstart
def chat_with_groq(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=False, maxp=None):
    logging.debug("Groq: Summarization process starting...")
    # https://console.groq.com/docs/api-reference#chat-create
    try:
        logging.debug("Groq: Loading and validating configurations")
        loaded_config_data = load_and_log_configs()
        if loaded_config_data is None:
            logging.error("Failed to load configuration data")
            groq_api_key = None
        else:
            # Prioritize the API key passed as a parameter
            if api_key and api_key.strip():
                groq_api_key = api_key
                logging.info("Groq: Using API key provided as parameter")
            else:
                # If no parameter is provided, use the key from the config
                groq_api_key = loaded_config_data['groq_api'].get('api_key')
                if groq_api_key:
                    logging.info("Groq: Using API key from config file")
                else:
                    logging.warning("Groq: No API key found in config file")

        # Final check to ensure we have a valid API key
        if not groq_api_key or not groq_api_key.strip():
            logging.error("Anthropic: No valid API key available")


        logging.debug(f"Groq: Using API Key: {groq_api_key[:5]}...{groq_api_key[-5:]}")

        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data.get('groq_api', {}).get('streaming', False)
            logging.debug("Groq: Streaming mode enabled")
        else:
            logging.debug("Groq: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        # Transcript data handling & Validation
        if isinstance(input_data, str) and os.path.isfile(input_data):
            logging.debug("Groq: Loading json data for summarization")
            with open(input_data, 'r') as file:
                data = json.load(file)
        else:
            logging.debug("Groq: Using provided string data for summarization")
            data = input_data

        # DEBUG - Debug logging to identify sent data
        logging.debug(f"Groq: Loaded data: {data[:500]}...(snipped to first 500 chars)")
        logging.debug(f"Groq: Type of data: {type(data)}")

        if isinstance(data, dict) and 'summary' in data:
            # If the loaded data is a dictionary and already contains a summary, return it
            logging.debug("Groq: Summary already exists in the loaded data")
            return data['summary']

        # If the loaded data is a list of segment dictionaries or a string, proceed with summarization
        if isinstance(data, list):
            segments = data
            text = extract_text_from_segments(segments)
        elif isinstance(data, str):
            text = data
        else:
            raise ValueError("Groq: Invalid input data format")

        # Set the model to be used
        groq_model = loaded_config_data['groq_api']['model']

        if temp is None:
            temp = 0.2
        temp = float(temp)
        if system_message is None:
            system_message = "You are a helpful AI assistant who does whatever the user requests."

        headers = {
            'Authorization': f'Bearer {groq_api_key}',
            'Content-Type': 'application/json'
        }

        groq_prompt = f"{text} \n\n\n\n{custom_prompt_arg}"
        logging.debug("groq: Prompt being sent is {groq_prompt}")

        data = {
            "messages": [
                {
                    "role": "system",
                    "content": system_message,
                },
                {
                    "role": "user",
                    "content": groq_prompt,
                }
            ],
            "model": groq_model,
            "temperature": temp,
            "stream": streaming,
            "top_p": maxp
        }

        logging.debug("groq: Submitting request to API endpoint")
        print("groq: Submitting request to API endpoint")
        if streaming:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['groq_api']['api_retries']
            retry_delay = loaded_config_data['groq_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            response = session.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=data,
                stream=True  # Enable response streaming
            )
            response.raise_for_status()

            def stream_generator():
                collected_messages = ""
                for line in response.iter_lines():
                    line = line.decode("utf-8").strip()

                    if line == "":
                        continue

                    if line.startswith("data: "):
                        data_str = line[len("data: "):]
                        if data_str == "[DONE]":
                            break
                        try:
                            data_json = json.loads(data_str)
                            chunk = data_json["choices"][0]["delta"].get("content", "")
                            collected_messages += chunk
                            yield chunk
                        except json.JSONDecodeError:
                            logging.error(f"Groq: Error decoding JSON from line: {line}")
                            continue
                # Optionally, you can return the full collected message at the end
                # yield collected_messages

            return stream_generator()
        else:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['groq_api']['api_retries']
            retry_delay = loaded_config_data['groq_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            response = session.post(
                'https://api.groq.com/openai/v1/chat/completions',
                headers=headers,
                json=data
            )

            response_data = response.json()
            logging.debug(f"API Response Data: {response_data}")

            if response.status_code == 200:
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    summary = response_data['choices'][0]['message']['content'].strip()
                    logging.debug("Groq: Summarization successful")
                    return summary
                else:
                    logging.error("Groq: Expected data not found in API response.")
                    return "Groq: Expected data not found in API response."
            else:
                logging.error(f"Groq: API request failed with status code {response.status_code}: {response.text}")
                return f"Groq: API request failed: {response.text}"

    except Exception as e:
        logging.error("Groq: Error in processing: %s", str(e), exc_info=True)
        return f"Groq: Error occurred while processing summary with Groq: {str(e)}"


def chat_with_openrouter(api_key=None, input_data=None, custom_prompt_arg=None, temp=None, system_message=None, streaming=False, top_p=None, top_k=None, minp=None):
    import requests
    import json
    openrouter_system_prompt = "You are a helpful AI assistant who does whatever the user requests."
    # https://openrouter.ai/docs/requests
    try:
        logging.info("OpenRouter: Loading and validating configurations")
        loaded_config_data = load_and_log_configs()
        if loaded_config_data is None:
            logging.error("Failed to load configuration data")
            return "OpenRouter: Failed to load configuration data"

        # Prioritize the API key passed as a parameter
        if api_key and isinstance(api_key, str) and api_key.strip():
            logging.debug(f"OpenRouter: Using API key {api_key[:5]}...{api_key[-5:]} provided as parameter")
            print(f"API Key: {api_key}")
            print("FUCK THIS2")
        else:
            # Fall back to the API key from the config file
            api_key = loaded_config_data['openrouter_api'].get('api_key')
            if api_key:
                logging.debug(f"OpenRouter: Loaded API Key from config: {api_key[:5]}...{api_key[-5:]}")
            else:
                logging.error("OpenRouter: No API key found in config file")
                return "OpenRouter: No API key found in config file"

        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data.get('openrouter_api', {}).get('streaming', False)
            logging.debug("OpenRouter: Streaming mode enabled")
        else:
            logging.debug("OpenRouter: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        # Model Selection validation
        openrouter_model = loaded_config_data['openrouter_api']['model']
        logging.debug(f"OpenRouter: Using model from config file: {openrouter_model}")

        print(f"API Key: {api_key}")
        print("FUCK THIS3")

        # Top-P
        if not top_p:
            logging.debug(f"OpenRouter: Using top_p {top_p} from config file")
            top_p = loaded_config_data['openrouter_api']['top_p']
            top_p = float(top_p)

        # Top-K
        if not top_k:
            logging.debug(f"OpenRouter: Using top_k {top_k} from config file")
            top_k = loaded_config_data['openrouter_api']['top_k']
            top_k = float(top_k)

        # Min-P
        if not minp:
            logging.debug(f"OpenRouter: Using min_p {minp} from config file")
            minp = loaded_config_data['openrouter_api']['min_p']
            minp = float(minp)

        if isinstance(input_data, str) and os.path.isfile(input_data):
            logging.debug("OpenRouter: Loading json data for summarization")
            with open(input_data, 'r') as file:
                data = json.load(file)
        else:
            logging.debug("OpenRouter: Using provided string data for summarization")
            data = input_data

        # DEBUG - Debug logging to identify sent data
        logging.debug(f"OpenRouter: Loaded data: {data[:500]}...(snipped to first 500 chars)")
        logging.debug(f"OpenRouter: Type of data: {type(data)}")

        if isinstance(data, dict) and 'summary' in data:
            # If the loaded data is a dictionary and already contains a summary, return it
            logging.debug("OpenRouter: Summary already exists in the loaded data")
            return data['summary']

        # If the loaded data is a list of segment dictionaries or a string, proceed with summarization
        if isinstance(data, list):
            segments = data
            text = extract_text_from_segments(segments)
        elif isinstance(data, str):
            text = data
        else:
            raise ValueError("OpenRouter: Invalid input data format")

        # System Prompt
        if not isinstance(system_message, str):
            logging.debug("OpenRouter: Using default system prompt")
            system_message = openrouter_system_prompt
        else:
            logging.debug("OpenRouter: Using provided system prompt")

        openrouter_prompt = f"{input_data} \n\n\n\n{custom_prompt_arg}"
        logging.debug(f"openrouter: User Prompt being sent is {openrouter_prompt}")

    except Exception as e:
        logging.error(f"OpenRouter: Error in processing: {str(e)}")
        return f"OpenRouter: Error occurred while processing config file with OpenRouter: {str(e)}"

    if streaming:
        try:
            logging.debug("OpenRouter: Submitting streaming request to API endpoint")
            print("OpenRouter: Submitting streaming request to API endpoint")

            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['openrouter_api']['api_retries']
            retry_delay = loaded_config_data['openrouter_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            # Make streaming request
            response = session.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Accept": "text/event-stream",  # Important for streaming
                },
                data=json.dumps({
                    "model": openrouter_model,
                    "messages": [
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": openrouter_prompt}
                    ],
                    #"max_tokens": 4096,
                    "top_p": top_p,
                    "top_k": top_k,
                    "min_p": minp,
                    "temperature": temp,
                    "stream": True
                }),
            )
            logging.debug(f"Raw API Response Content: {response.text}")
            if response.status_code == 200:
                full_response = ""
                # Process the streaming response
                for line in response.iter_lines():
                    if line:
                        # Remove "data: " prefix and parse JSON
                        line = line.decode('utf-8')
                        if line.startswith('data: '):
                            json_str = line[6:]  # Remove "data: " prefix
                            if json_str.strip() == '[DONE]':
                                break
                            try:
                                json_data = json.loads(json_str)
                                if 'choices' in json_data and len(json_data['choices']) > 0:
                                    delta = json_data['choices'][0].get('delta', {})
                                    if 'content' in delta:
                                        content = delta['content']
                                        print(content, end='', flush=True)  # Print streaming output
                                        full_response += content
                            except json.JSONDecodeError as e:
                                logging.error(f"Error decoding JSON chunk: {e}")
                                continue

                logging.debug("openrouter: Streaming completed successfully")
                return full_response.strip()
            else:
                error_msg = f"openrouter: Streaming API request failed with status code {response.status_code}: {response.text}"
                logging.error(error_msg)
                return error_msg

        except Exception as e:
            error_msg = f"openrouter: Error occurred while processing stream: {str(e)}"
            logging.error(error_msg)
            return error_msg
    else:
        try:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['openrouter_api']['api_retries']
            retry_delay = loaded_config_data['openrouter_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            logging.info("OpenRouter: Submitting request to API endpoint")
            response = session.post(
                url="https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                },
                data=json.dumps({
                    "model": openrouter_model,
                    "messages": [
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": openrouter_prompt}
                    ],
                    "top_p": top_p,
                    "top_k": top_k,
                    "min_p": minp,
                    "temperature": temp,
                    "stream": False
                }),
            )
            # Log response content
            logging.debug(f"Raw API Response Content: {response.text}")

            response_data = response.json()
            logging.debug(f"Full API Response Data: {response_data}")

            if response.status_code == 200:
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    summary = response_data['choices'][0]['message']['content'].strip()
                    logging.debug("openrouter: Chat request successful")
                    logging.debug("openrouter: Chat request successful.")
                    return summary
                else:
                    logging.error("openrouter: Expected data not found in API response.")
                    return "openrouter: Expected data not found in API response."
            else:
                logging.error(f"openrouter:  API request failed with status code {response.status_code}: {response.text}")
                return f"openrouter: API request failed: {response.text}"
        except Exception as e:
            logging.error(f"openrouter: Error in processing: {str(e)}")
            return f"openrouter: Error occurred while processing chat request with openrouter: {str(e)}"


def chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, temp=None, streaming=False):
    # https://huggingface.co/docs/api-inference/en/parameters
    loaded_config_data = load_and_log_configs()
    logging.debug(f"huggingface Chat: Chat request process starting...")
    try:
        # API key validation
        if not api_key or api_key.strip() == "":
            logging.info("HuggingFace Chat: API key not provided as parameter")
            logging.info("HuggingFace Chat: Attempting to use API key from config file")

        huggingface_api_key = loaded_config_data['huggingface_api'].get('api_key')
        logging.debug(f"HuggingFace Chat: API key from config: {huggingface_api_key[:5]}...{huggingface_api_key[-5:]}")

        if huggingface_api_key is None or huggingface_api_key.strip() == "":
            logging.error("HuggingFace Chat: API key not found or is empty")
            return "HuggingFace Chat: API Key Not Provided/Found in Config file or is empty"
        if huggingface_api_key:
            logging.info("HuggingFace Chat: Using API key from config file")
        headers = {
            "Authorization": f"Bearer {huggingface_api_key}"
        }

        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data.get('huggingface_api', {}).get('streaming', False)
            logging.debug("HuggingFace: Streaming mode enabled")
        else:
            logging.debug("HuggingFace: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        # Setup model
        huggingface_model = loaded_config_data['huggingface_api']['model']

        API_URL = f"https://api-inference.huggingface.co/models/{huggingface_model}/v1/chat/completions"
        if temp is None:
            temp = 1.0
        temp = float(temp)
        huggingface_prompt = f"{custom_prompt_arg}\n\n\n{input_data}"
        logging.debug(f"HuggingFace chat: Prompt being sent is {huggingface_prompt}")
        data = {
            "model": f"{huggingface_model}",
            "messages": [{"role": "user", "content": f"{huggingface_prompt}"}],
            "max_tokens": 4096,
            "stream": False,
            "temperature": temp
        }

        logging.debug("HuggingFace Chat: Submitting request...")
        if streaming:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['huggingface_api']['api_retries']
            retry_delay = loaded_config_data['huggingface_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            response = session.post(API_URL, headers=headers, json=data, stream=True)
            response.raise_for_status()

            def stream_generator():
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8').strip()
                        if decoded_line.startswith('data:'):
                            data_str = decoded_line[len('data:'):].strip()
                            if data_str == '[DONE]':
                                break
                            try:
                                data_json = json.loads(data_str)
                                if 'token' in data_json:
                                    token_text = data_json['token'].get('text', '')
                                    yield token_text
                                elif 'generated_text' in data_json:
                                    # Some models may send the full generated text
                                    generated_text = data_json['generated_text']
                                    yield generated_text
                                else:
                                    logging.debug(f"HuggingFace: Unhandled streaming data: {data_json}")
                            except json.JSONDecodeError:
                                logging.error(f"HuggingFace: Error decoding JSON from line: {decoded_line}")
                                continue
                # Optionally, yield the final collected text
                # yield collected_text

            return stream_generator()
        else:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['huggingface_api']['api_retries']
            retry_delay = loaded_config_data['huggingface_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            response = session.post(API_URL, headers=headers, json=data)
            logging.debug(f"Full API response data: {response.text}")

            if response.status_code == 200:
                response_json = response.json()
                if "choices" in response_json and len(response_json["choices"]) > 0:
                    generated_text = response_json["choices"][0]["message"]["content"]
                    logging.debug("HuggingFace Chat: Chat request successful")
                    print("HuggingFace Chat: Chat request successful.")
                    return generated_text.strip()
                else:
                    logging.error("HuggingFace Chat: No generated text in the response")
                    return "HuggingFace Chat: No generated text in the response"
            else:
                logging.error(
                    f"HuggingFace Chat: Chat request failed with status code {response.status_code}: {response.text}")
                return f"HuggingFace Chat: Failed to process chat request, status code {response.status_code}: {response.text}"
    except Exception as e:
        logging.error(f"HuggingFace Chat: Error in processing: {str(e)}")
        print(f"HuggingFace Chat: Error occurred while processing chat request with huggingface: {str(e)}")
        return None


def chat_with_deepseek(api_key, input_data, custom_prompt_arg, temp=0.1, system_message=None, streaming=False, topp=None):
    """
    Interacts with the DeepSeek API to generate summaries based on input data.

    Parameters:
        api_key (str): DeepSeek API key. If not provided, the key from the config is used.
        input_data (str or list): The data to summarize. Can be a string or a list of segments.
        custom_prompt_arg (str): Custom prompt to append to the input data.
        temp (float, optional): Temperature setting for the model. Defaults to 0.1.
        system_message (str, optional): System prompt for the assistant. Defaults to a helpful assistant message.
        max_retries (int, optional): Maximum number of retries for failed API calls. Defaults to 3.
        retry_delay (int, optional): Delay between retries in seconds. Defaults to 5.

    Returns:
        str: The summary generated by DeepSeek or an error message.
    """
    # https://api-docs.deepseek.com/api/create-chat-completion
    logging.debug("DeepSeek: Summarization process starting...")
    max_retries = 3
    retry_delay = 5
    try:
        logging.debug("DeepSeek: Loading and validating configurations")
        loaded_config_data = load_and_log_configs()
        if loaded_config_data is None:
            logging.error("DeepSeek: Failed to load configuration data")
            return "DeepSeek: Failed to load configuration data."

        # Prioritize the API key passed as a parameter
        if api_key and api_key.strip():
            deepseek_api_key = api_key.strip()
            logging.info("DeepSeek: Using API key provided as parameter")
        else:
            # If no parameter is provided, use the key from the config
            deepseek_api_key = loaded_config_data['deepseek_api'].get('api_key')
            if deepseek_api_key and deepseek_api_key.strip():
                deepseek_api_key = deepseek_api_key.strip()
                logging.info("DeepSeek: Using API key from config file")
            else:
                logging.error("DeepSeek: No valid API key available")
                return "DeepSeek: API Key Not Provided/Found in Config file or is empty"

        logging.debug("DeepSeek: Using API Key")

        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data.get('deepseek_api', {}).get('streaming', False)
            logging.debug("DeepSeek: Streaming mode enabled")
        else:
            logging.debug("DeepSeek: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        # Set default system prompt if not provided
        if isinstance(system_message, str):
            logging.debug("DeepSeek: Using provided system prompt")
            deepseek_system_message = system_message
        else:
            deepseek_system_message = "You are a helpful AI assistant who does whatever the user requests."
            logging.debug("DeepSeek Chat: Using default system prompt")

        # Input data handling
        if isinstance(input_data, str) and os.path.isfile(input_data):
            logging.debug("DeepSeek: Loading JSON data for summarization")
            with open(input_data, 'r', encoding='utf-8') as file:
                try:
                    data = json.load(file)
                except json.JSONDecodeError as e:
                    logging.error(f"DeepSeek: JSON decoding failed: {str(e)}")
                    data = input_data
                    pass
        else:
            logging.debug("DeepSeek: Using provided string data for summarization")
            data = input_data

        # DEBUG - Debug logging to identify sent data
        if isinstance(data, str):
            snipped_data = data[:500] + "..." if len(data) > 500 else data
            logging.debug(f"DeepSeek: Loaded data (snipped to first 500 chars): {snipped_data}")
        elif isinstance(data, list):
            snipped_data = json.dumps(data[:2], indent=2) + "..." if len(data) > 2 else json.dumps(data, indent=2)
            logging.debug(f"DeepSeek: Loaded data (snipped to first 2 segments): {snipped_data}")
        else:
            logging.debug(f"DeepSeek: Loaded data: {data}")

        logging.debug(f"DeepSeek: Type of data: {type(data)}")

        if isinstance(data, dict) and 'summary' in data:
            # If the loaded data is a dictionary and already contains a summary, return it
            logging.debug("DeepSeek: Summary already exists in the loaded data")
            return data['summary']

        # Text extraction
        if isinstance(data, list):
            segments = data
            try:
                text = extract_text_from_segments(segments)
                logging.debug("DeepSeek: Extracted text from segments")
            except Exception as e:
                logging.error(f"DeepSeek: Error extracting text from segments: {str(e)}")
                return f"DeepSeek: Error extracting text from segments: {str(e)}"
        elif isinstance(data, str):
            text = data
            logging.debug("DeepSeek: Using string data directly")
        else:
            raise ValueError("DeepSeek: Invalid input data format")

        # Retrieve the model from config if not provided
        deepseek_model = loaded_config_data['deepseek_api'].get('deepseek', "deepseek-chat")
        logging.debug(f"DeepSeek: Using model: {deepseek_model}")

        # Ensure temperature is a float within acceptable range
        try:
            temp = float(temp)
            if not (0.0 <= temp <= 1.0):
                logging.warning("DeepSeek: Temperature out of bounds (0.0 - 1.0). Setting to default 0.1")
                temp = 0.1
        except (ValueError, TypeError):
            logging.warning("DeepSeek: Invalid temperature value. Setting to default 0.1")
            temp = 0.1

        headers = {
            'Authorization': f'Bearer {deepseek_api_key}',
            'Content-Type': 'application/json'
        }

        logging.debug("DeepSeek: Preparing data and prompt for submittal")
        deepseek_prompt = f"{text}\n\n\n\n{custom_prompt_arg}"
        payload = {
            "model": deepseek_model,
            "messages": [
                {"role": "system", "content": deepseek_system_message},
                {"role": "user", "content": deepseek_prompt}
            ],
            "stream": streaming,
            "temperature": temp,
            "top_p": topp
        }

        logging.debug("DeepSeek: Posting request to API")

        try:
            if streaming:
                # Create a session
                session = requests.Session()

                # Load config values
                retry_count = loaded_config_data['deepseek_api']['api_retries']
                retry_delay = loaded_config_data['deepseek_api']['api_retry_delay']

                # Configure the retry strategy
                retry_strategy = Retry(
                    total=retry_count,  # Total number of retries
                    backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                    status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
                )

                # Create the adapter
                adapter = HTTPAdapter(max_retries=retry_strategy)

                # Mount adapters for both HTTP and HTTPS
                session.mount("http://", adapter)
                session.mount("https://", adapter)
                logging.debug("DeepSeek: Posting streaming request")
                response = session.post(
                    'https://api.deepseek.com/chat/completions',
                    headers=headers,
                    json=payload,
                    stream=True
                )
                response.raise_for_status()

                def stream_generator():
                    collected_text = ""
                    for line in response.iter_lines():
                        if line:
                            decoded_line = line.decode('utf-8').strip()
                            if decoded_line == '':
                                continue
                            if decoded_line.startswith('data: '):
                                data_str = decoded_line[len('data: '):]
                                if data_str == '[DONE]':
                                    break
                                try:
                                    data_json = json.loads(data_str)
                                    delta_content = data_json['choices'][0]['delta'].get('content', '')
                                    collected_text += delta_content
                                    yield delta_content
                                except json.JSONDecodeError:
                                    logging.error(f"DeepSeek: Error decoding JSON from line: {decoded_line}")
                                    continue
                                except KeyError as e:
                                    logging.error(f"DeepSeek: Key error: {str(e)} in line: {decoded_line}")
                                    continue
                    yield collected_text

                return stream_generator()
            else:
                # Create a session
                session = requests.Session()

                # Load config values
                retry_count = loaded_config_data['deepseek_api']['api_retries']
                retry_delay = loaded_config_data['deepseek_api']['api_retry_delay']

                # Configure the retry strategy
                retry_strategy = Retry(
                    total=retry_count,  # Total number of retries
                    backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                    status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
                )

                # Create the adapter
                adapter = HTTPAdapter(max_retries=retry_strategy)

                # Mount adapters for both HTTP and HTTPS
                session.mount("http://", adapter)
                session.mount("https://", adapter)
                response = session.post('https://api.deepseek.com/chat/completions', headers=headers, json=payload, timeout=30)
                logging.debug(f"DeepSeek: Full API response: {response.status_code} - {response.text}")

                if response.status_code == 200:
                    response_data = response.json()
                    logging.debug(f"DeepSeek: Response JSON: {json.dumps(response_data, indent=2)}")

                    # Adjust parsing based on actual API response structure
                    if 'choices' in response_data:
                        if len(response_data['choices']) > 0:
                            summary = response_data['choices'][0]['message']['content'].strip()
                            logging.debug("DeepSeek: Chat request successful")
                            return summary
                        else:
                            logging.error("DeepSeek: 'choices' key is empty in response")
                    else:
                        logging.error("DeepSeek: 'choices' key missing in response")
                    return "DeepSeek: Unexpected response format from API."
                elif 500 <= response.status_code < 600:
                    logging.error(f"DeepSeek: Server error (status code {response.status_code})")
                else:
                    logging.error(f"DeepSeek: Request failed with status code {response.status_code}. Response: {response.text}")
                    return f"DeepSeek: Failed to process chat request. Status code: {response.status_code}"

        except requests.Timeout:
            logging.error(f"DeepSeek: Request timed out.")
        except requests.RequestException as e:
            logging.error(f"DeepSeek: Request exception occurred: {str(e)}.")
    except Exception as e:
        logging.error(f"DeepSeek: Unexpected error in processing: {str(e)}", exc_info=True)
        return f"DeepSeek: Error occurred while processing chat request: {str(e)}"


def chat_with_mistral(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=False, topp=None, model=None):
    logging.debug("Mistral: Chat request made")
    try:
        logging.debug("Mistral: Loading and validating configurations")
        loaded_config_data = load_and_log_configs()
        if loaded_config_data is None:
            logging.error("Failed to load configuration data")
            mistral_api_key = None
        else:
            # Prioritize the API key passed as a parameter
            if api_key and api_key.strip():
                mistral_api_key = api_key
                logging.info("Mistral: Using API key provided as parameter")
            else:
                # If no parameter is provided, use the key from the config
                mistral_api_key = loaded_config_data['mistral_api'].get('api_key')
                if mistral_api_key:
                    logging.info("Mistral: Using API key from config file")
                else:
                    logging.warning("Mistral: No API key found in config file")

        # Final check to ensure we have a valid API key
        if not mistral_api_key or not mistral_api_key.strip():
            logging.error("Mistral: No valid API key available")
            return "Mistral: API Key Not Provided/Found in Config file or is empty"

        logging.debug(f"Mistral: Using API Key: {mistral_api_key[:5]}...{mistral_api_key[-5:]}")

        # Streaming mode validation
        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data.get('mistral_api', {}).get('streaming', False)
            logging.debug("Mistral: Streaming mode enabled")
        else:
            logging.debug("Mistral: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        # Model Selection validation
        if isinstance(model, str):
            mistral_model = model
        else:
            mistral_model = loaded_config_data['mistral_api'].get('model', "mistral-large-latest")

        # Temp validation
        if isinstance(temp, float):
            temp = float(temp)
        else:
            temp = loaded_config_data['mistral_api'].get('temperature', 0.1)
            temp = float(temp)
            logging.debug("Mistral: Using temperature from config file")

        # Top-P validation
        if isinstance(topp, float):
            topp = float(topp)
        else:
            topp = loaded_config_data['mistral_api'].get('top_p', 0.95)
            topp = float(topp)
            logging.debug(f"Mistral: Using Top-P: {topp} from config file")

        # System message validation
        if system_message is None:
            mistral_system_message = "You are a helpful AI assistant who does whatever the user requests."
            system_message = mistral_system_message

        # Input data handling
        logging.debug("Mistral: Using provided string data")
        data = input_data

        # Text extraction
        if isinstance(input_data, list):
            text = extract_text_from_segments(input_data)
        elif isinstance(input_data, str):
            text = input_data
        else:
            raise ValueError("Mistral: Invalid input data format")

        headers = {
            'Authorization': f'Bearer {mistral_api_key}',
            'Content-Type': 'application/json'
        }

        logging.debug(
            f"Deepseek API Key: {mistral_api_key[:5]}...{mistral_api_key[-5:] if mistral_api_key else None}")
        logging.debug("Mistral: Preparing data + prompt for submittal")
        mistral_prompt = f"{custom_prompt_arg}\n\n\n\n{text} "
        data = {
            "model": mistral_model,
            "messages": [
                {"role": "system",
                 "content": system_message},
                {"role": "user",
                "content": mistral_prompt}
            ],
            "temperature": temp,
            "top_p": topp,
            # FIXME - Add global max tokens variable
            "max_tokens": 4096,
            "stream": streaming,
            "safe_prompt": False
        }

        if streaming:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['mistral_api']['api_retries']
            retry_delay = loaded_config_data['mistral_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            logging.debug("Mistral: Posting streaming request")
            response = session.post(
                'https://api.mistral.ai/v1/chat/completions',
                headers=headers,
                json=data,
                stream=True
            )
            response.raise_for_status()

            def stream_generator():
                collected_text = ""
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8').strip()
                        if decoded_line == '':
                            continue
                        try:
                            # Assuming the response is in SSE format
                            if decoded_line.startswith('data:'):
                                data_str = decoded_line[len('data:'):].strip()
                                if data_str == '[DONE]':
                                    break
                                data_json = json.loads(data_str)
                                if 'choices' in data_json and len(data_json['choices']) > 0:
                                    delta_content = data_json['choices'][0]['delta'].get('content', '')
                                    collected_text += delta_content
                                    yield delta_content
                                else:
                                    logging.error(f"Mistral: Unexpected data format: {data_json}")
                                    continue
                            else:
                                # Handle other event types if necessary
                                continue
                        except json.JSONDecodeError:
                            logging.error(f"Mistral: Error decoding JSON from line: {decoded_line}")
                            continue
                        except KeyError as e:
                            logging.error(f"Mistral: Key error: {str(e)} in line: {decoded_line}")
                            continue
                # Optionally, you can return the full collected text at the end
                # yield collected_text
            return stream_generator()
        else:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['mistral_api']['api_retries']
            retry_delay = loaded_config_data['mistral_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            logging.debug("Mistral: Posting non-streaming request")
            response = session.post(
                'https://api.mistral.ai/v1/chat/completions',
                headers=headers,
                json=data
            )

            if response.status_code == 200:
                response_data = response.json()
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    summary = response_data['choices'][0]['message']['content'].strip()
                    logging.debug("Mistral: Chat Request successful")
                    return summary
                else:
                    logging.warning("Mistral: Chat response not found in the response data")
                    return "Mistral: Chat response not available"
            else:
                logging.error(f"Mistral: Chat request failed with status code {response.status_code}")
                logging.error(f"Mistral: Error response: {response.text}")
                return f"Mistral: Failed to process Chat response. Status code: {response.status_code}"
    except Exception as e:
        logging.error(f"Mistral: Error in processing: {str(e)}", exc_info=True)
        return f"Mistral: Error occurred while processing Chat: {str(e)}"


def chat_with_google(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=False, topp=None, topk=None):
    # https://ai.google.dev/api/generate-content#v1beta.GenerationConfig
    loaded_config_data = load_and_log_configs()
    google_api_key = api_key
    try:
        # API key validation
        if not google_api_key:
            logging.info("Google: API key not provided as parameter")
            logging.info("Google: Attempting to use API key from config file")
            google_api_key = loaded_config_data['google_api']['api_key']

        if not google_api_key:
            logging.error("Google: API key not found or is empty")
            return "Google: API Key Not Provided/Found in Config file or is empty"

        logging.debug(f"Google: Using API Key: {google_api_key[:5]}...{google_api_key[-5:]}")

        if isinstance(streaming, str):
            streaming = streaming.lower() == "true"
        elif isinstance(streaming, int):
            streaming = bool(streaming)  # Convert integers (1/0) to boolean
        elif streaming is None:
            streaming = loaded_config_data.get('google_api', {}).get('streaming', False)
            logging.debug("Google: Streaming mode enabled")
        else:
            logging.debug("Google: Streaming mode disabled")
        if not isinstance(streaming, bool):
            raise ValueError(f"Invalid type for 'streaming': Expected a boolean, got {type(streaming).__name__}")

        if isinstance(temp, float):
            temp = float(temp)
        else:
            temp = loaded_config_data['google_api'].get('temperature', 0.1)

        if isinstance(system_message, str):
            logging.debug("Google: Using provided system message")
            system_message = system_message
        else:
            google_system_message = "You are a helpful AI assistant who does whatever the user requests."
            logging.debug("Google: Using default system message")
            system_message = google_system_message

        if isinstance(topp, float):
            topp = float(topp)
            logging.debug(f"Google: Using top_p {topp} from parameter")
        else:
            topp = loaded_config_data['google_api'].get('top_p', 0.9)
            logging.debug(f"Google: Using top_p {topp} from config file")

        if isinstance(topk, int):
            topk = int(topk)
        else:
            topk = loaded_config_data['google_api'].get('top_k', 100)



        # Model selection
        google_model = loaded_config_data['google_api']['model'] or "gemini-1.5-pro"
        logging.debug(f"Google: Using model: {google_model}")

        # Input data handling
        logging.debug(f"Google: Raw input data type: {type(input_data)}")
        logging.debug(f"Google: Raw input data (first 500 chars): {str(input_data)[:500]}...")

        if isinstance(input_data, str):
            if input_data.strip().startswith('{'):
                # It's likely a JSON string
                logging.debug("Google: Parsing provided JSON string data for chat message")
                try:
                    data = json.loads(input_data)
                except json.JSONDecodeError as e:
                    logging.error(f"Google: Error parsing JSON string: {str(e)}")
                    data = input_data
                    pass
            elif os.path.isfile(input_data):
                logging.debug("Google: Loading JSON data from file for chat message")
                with open(input_data, 'r') as file:
                    data = json.load(file)
            else:
                logging.debug("Google: Using provided string data for chat message")
                data = input_data
        else:
            data = input_data

        logging.debug(f"Google: Processed data type: {type(data)}")
        logging.debug(f"Google: Processed data (first 500 chars): {str(data)[:500]}...")

        # Text extraction
        if isinstance(data, dict):
            if 'summary' in data:
                logging.debug("Google: Summary already exists in the loaded data")
                return data['summary']
            elif 'segments' in data:
                text = extract_text_from_segments(data['segments'])
            else:
                text = json.dumps(data)  # Convert dict to string if no specific format
        elif isinstance(data, list):
            text = extract_text_from_segments(data)
        elif isinstance(data, str):
            text = data
        else:
            raise ValueError(f"Google: Invalid input data format: {type(data)}")

        logging.debug(f"Google: Extracted text (first 500 chars): {text[:500]}...")
        logging.debug(f"Google: Custom prompt: {custom_prompt_arg}")


        headers = {
            'Authorization': f'Bearer {google_api_key}',
            'Content-Type': 'application/json',
            "Accept": "text/event-stream" if streaming else "application/json"

        }

        logging.debug(
            f"Google API Key: {google_api_key[:5]}...{google_api_key[-5:] if google_api_key else None}")
        logging.debug("Google: Preparing data + prompt for submittal")
        google_prompt = f"{text} \n\n\n\n{custom_prompt_arg}"

        google_max_tokens = 4096

        data = {
            "model": google_model,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": google_prompt}
            ],
            "temperature": temp,
            "topP": topp,
            #"topK": topk,
            #"maxOutputTokens": google_max_tokens,
        }

        if streaming:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['google_api']['api_retries']
            retry_delay = loaded_config_data['google_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            logging.debug("Google: Posting streaming request")
            response = session.post(
                "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions",
                headers=headers,
                json=data,
                stream=True
            )
            response.raise_for_status()

            def stream_generator():
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8').strip()
                        if decoded_line == '':
                            continue
                        if decoded_line.startswith('data: '):
                            data_str = decoded_line[len('data: '):]
                            if data_str == '[DONE]':
                                break
                            try:
                                data_json = json.loads(data_str)
                                chunk = data_json["choices"][0]["delta"].get("content", "")
                                yield chunk
                            except json.JSONDecodeError:
                                logging.error(f"Google: Error decoding JSON from line: {decoded_line}")
                                continue
                            except KeyError as e:
                                logging.error(f"Google: Key error: {str(e)} in line: {decoded_line}")
                                continue
            return stream_generator()
        else:
            # Create a session
            session = requests.Session()

            # Load config values
            retry_count = loaded_config_data['google_api']['api_retries']
            retry_delay = loaded_config_data['google_api']['api_retry_delay']

            # Configure the retry strategy
            retry_strategy = Retry(
                total=retry_count,  # Total number of retries
                backoff_factor=retry_delay,  # A delay factor (exponential backoff)
                status_forcelist=[429, 502, 503, 504],  # Status codes to retry on
            )

            # Create the adapter
            adapter = HTTPAdapter(max_retries=retry_strategy)

            # Mount adapters for both HTTP and HTTPS
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            logging.debug("Google: Posting request")
            response = session.post('https://generativelanguage.googleapis.com/v1beta/openai/chat/completions', headers=headers, json=data)
            logging.debug(f"Full API response data: {response}")
            if response.status_code == 200:
                response_data = response.json()
                logging.debug(response_data)
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    chat_response = response_data['choices'][0]['message']['content'].strip()
                    logging.debug("Google: Chat Sent successfully")
                    logging.debug(f"Google: Chat response: {chat_response}")
                    return chat_response
                else:
                    logging.warning("Google: Chat response not found in the response data")
                    return "Google: Chat not available"
            else:
                logging.error(f"Google: Chat request failed with status code {response.status_code}")
                logging.error(f"Google: Error response: {response.text}")
                return f"Google: Failed to process chat response. Status code: {response.status_code}"
    except json.JSONDecodeError as e:
        logging.error(f"Google: Error decoding JSON: {str(e)}", exc_info=True)
        return f"Google: Error decoding JSON input: {str(e)}"
    except requests.RequestException as e:
        logging.error(f"Google: Error making API request: {str(e)}", exc_info=True)
        return f"Google: Error making API request: {str(e)}"
    except Exception as e:
        logging.error(f"Google: Unexpected error: {str(e)}", exc_info=True)
        return f"Google: Unexpected error occurred: {str(e)}"

#
#
#######################################################################################################################