# chat_request_models.py
# Description: This code provides schema models for the /chat API endpoints
#
# Imports
# import os # No longer needed for getenv here
import logging  # For potential warnings
from typing import Optional, Dict, Any, Literal, Union, List
# import toml # No longer needed here

#
# 3rd-party imports
from pydantic import BaseModel, ConfigDict, Field, HttpUrl, field_validator, model_validator

#
# Local Imports
# Assuming 'tldw_cli' is a top-level package accessible in PYTHONPATH
# Adjust the import path if your project structure is different.
try:
    from tldw_cli.config import get_default_llm_provider, get_supported_api_providers
except ImportError:
    # Fallback if tldw_cli.config is not found, e.g. during unit tests or isolated use
    # This is a basic fallback and might not be suitable for all environments.
    logging.error("Could not import from tldw_cli.config. Using minimal fallbacks for schema generation.")


    def get_default_llm_provider():
        return "openai"


    def get_supported_api_providers():
        return ["openai", "anthropic"]  # Minimal list

#######################################################################################################################
#



# --- Tool Definitions ---
class FunctionDefinition(BaseModel):
    name: str = Field(...,
                      description="The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.")
    description: Optional[str] = Field(None,
                                       description="A description of what the function does, used by the model to choose when and how to call the function.")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict,
                                                 description="The parameters the functions accepts, described as a JSON Schema object.")


class ToolDefinition(BaseModel):
    type: Literal["function"] = Field(..., description="The type of the tool. Currently, only `function` is supported.")
    function: FunctionDefinition


class ToolChoiceFunction(BaseModel):
    name: str = Field(..., description="The name of the function to call.")


class ToolChoiceOption(BaseModel):
    type: Literal["function"] = Field(..., description="The type of the tool. Currently, only `function` is supported.")
    function: ToolChoiceFunction


# --- Message Definitions ---
class ChatCompletionRequestMessageContentPartText(BaseModel):
    type: Literal["text"]
    text: str


class ChatCompletionRequestMessageContentPartImageURL(BaseModel):
    url: Union[HttpUrl, str] = Field(..., description="Either a URL of the image or the base64 encoded image data.")
    detail: Optional[Literal["auto", "low", "high"]] = Field("auto",
                                                             description="Specifies the detail level of the image.")

    @field_validator('url', mode='before')  # Use mode='before' if validating raw input
    def check_url_or_data(cls, v):
        if isinstance(v, str) and not v.startswith('data:image') and not (
                v.startswith('http://') or v.startswith('https://')):
            # Check if it's a valid HttpUrl if not a data URI
            try:
                HttpUrl(v)  # Will raise ValueError if not a valid URL
            except ValueError:
                raise ValueError('String url must be a valid HTTP/HTTPS URL or a data URI for base64 encoded images')
        return v


class ChatCompletionRequestMessageContentPartImage(BaseModel):
    type: Literal["image_url"]
    image_url: ChatCompletionRequestMessageContentPartImageURL


ChatCompletionRequestMessageContentPart = Union[
    ChatCompletionRequestMessageContentPartText,
    ChatCompletionRequestMessageContentPartImage
]


class BaseMessage(BaseModel):
    role: Literal["system", "user", "assistant", "tool"]
    name: Optional[str] = Field(None, description="An optional name for the participant.")


class ChatCompletionSystemMessageParam(BaseMessage):
    role: Literal["system"]
    content: str


class ChatCompletionUserMessageParam(BaseMessage):
    role: Literal["user"]
    content: Union[str, List[ChatCompletionRequestMessageContentPart]]


class FunctionCall(BaseModel):
    arguments: str = Field(...,
                           description="The arguments to call the function with, as generated by the model in JSON format.")
    name: str = Field(..., description="The name of the function to call.")


class ChatCompletionMessageToolCallParam(BaseModel):
    id: str = Field(..., description="The ID of the tool call.")
    type: Literal["function"] = Field(..., description="The type of the tool. Currently, only `function` is supported.")
    # function: FunctionDefinition # This should be FunctionCall or similar structure for what model *called*
    # OpenAI spec: function: {"name": "get_current_weather", "arguments": "{\n  \"location\": \"Boston, MA\"\n}"}
    function: FunctionCall  # Re-using FunctionCall model makes sense here as it has name and arguments (JSON string)


class ChatCompletionAssistantMessageParam(BaseMessage):
    role: Literal["assistant"]
    content: Optional[str] = Field(None,
                                   description="The contents of the assistant message. Required unless tool_calls or function_call is specified.")
    tool_calls: Optional[List[ChatCompletionMessageToolCallParam]] = Field(None,
                                                                           description="The tool calls generated by the model, such as function calls.")
    function_call: Optional[FunctionCall] = Field(None, deprecated=True,
                                                  description="Deprecated and replaced by `tool_calls`.")

    @model_validator(mode='before')
    @classmethod  # Ensure it's a classmethod if using Pydantic v2 style
    def check_content_or_tool_call(cls, values):
        content = values.get('content')
        tool_calls = values.get('tool_calls')
        function_call = values.get('function_call')
        if content is None and not tool_calls and not function_call:
            raise ValueError('Assistant message must have content or tool_calls (or deprecated function_call)')
        return values


class ChatCompletionToolMessageParam(BaseMessage):
    role: Literal["tool"]
    content: str = Field(..., description="The contents of the tool message.")
    tool_call_id: str = Field(..., description="Tool call that this message is responding to.")


ChatCompletionMessageParam = Union[
    ChatCompletionSystemMessageParam,
    ChatCompletionUserMessageParam,
    ChatCompletionAssistantMessageParam,
    ChatCompletionToolMessageParam,
]


class ResponseFormat(BaseModel):
    type: Literal["text", "json_object"] = Field("text", description="Must be one of `text` or `json_object`.")


# --- Main Request Model ---
class ChatCompletionRequest(BaseModel):
    model_config = model_config  # Apply the module-level pydantic config

    api_provider: Optional[SUPPORTED_API_ENDPOINTS] = Field(
        default=None,
        description=f"[Extension] The target LLM provider (e.g., 'openai', 'anthropic'). If omitted, defaults to the server's configured default ('{DEFAULT_LLM_PROVIDER}')."
    )
    model: Optional[str] = Field(None,
                                 description="ID of the model to use. Specific model compatibility depends on the selected `api_provider`.")  # Made optional, server can default
    messages: List[ChatCompletionMessageParam] = Field(...,
                                                       description="A list of messages comprising the conversation so far.",
                                                       min_length=1)
    frequency_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)
    logit_bias: Optional[Dict[str, float]] = Field(None)
    logprobs: Optional[bool] = Field(False)
    top_logprobs: Optional[int] = Field(None, ge=0, le=20)
    max_tokens: Optional[int] = Field(None)
    n: Optional[int] = Field(1, ge=1, le=128)
    presence_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)
    response_format: Optional[ResponseFormat] = Field(None)
    seed: Optional[int] = Field(None)
    stop: Optional[Union[str, List[str]]] = Field(None)
    stream: Optional[bool] = Field(False)
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0)
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0)
    tools: Optional[List[ToolDefinition]] = Field(None, max_length=128)
    tool_choice: Optional[Union[Literal["none", "auto", "required"], ToolChoiceOption]] = Field("auto")
    user: Optional[str] = Field(None)
    minp: Optional[float] = Field(None, description="[Extension] Minimum probability threshold (provider specific).")
    topk: Optional[int] = Field(None, description="[Extension] Top-K sampling parameter (provider specific).")
    prompt_template_name: Optional[str] = Field(None, description="Name of the prompt template to apply.")
    character_id: Optional[str] = Field(None, description="Optional ID of the character to use for context.")
    conversation_id: Optional[str] = Field(None, description="Optional ID of the conversation to use for context.")

    # Class Config was defined for old Pydantic v1. model_config is v2.
    # Removed `class Config: extra = "allow"` as it's handled by module `model_config`

    @model_validator(mode='before')
    @classmethod
    def check_logprobs(cls, values):
        logprobs = values.get('logprobs')
        top_logprobs = values.get('top_logprobs')
        if top_logprobs is not None and not logprobs:
            raise ValueError("If top_logprobs is specified, logprobs must be set to true.")
        return values

#
# End of chat_request_schemas.py
#######################################################################################################################

