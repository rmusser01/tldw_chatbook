# LLM_API_Calls.py
#########################################
# General LLM API Calling Library
# This library is used to perform API Calls against commercial LLM endpoints.
#
####
####################
# Function List
#
# 1. extract_text_from_segments(segments: List[Dict]) -> str
# 2. chat_with_openai(api_key, file_path, custom_prompt_arg, streaming=None)
# 3. chat_with_anthropic(api_key, file_path, model, custom_prompt_arg, max_retries=3, retry_delay=5, streaming=None)
# 4. chat_with_cohere(api_key, file_path, model, custom_prompt_arg, streaming=None)
# 5. chat_with_groq(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None):
# 6. chat_with_openrouter(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 7. chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 8. chat_with_deepseek(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
#
#
####################
#
# Import necessary libraries
import json
import logging
import os
import time
from typing import List, Any, Generator, Optional, Union
#
# Import 3rd-Party Libraries
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
#
# Import Local libraries
from ..config import get_setting
#
#######################################################################################################################
# Function Definitions
#

# --- Helper function for safe type conversion ---
def _safe_cast(value: Any, cast_to: type, default: Any = None) -> Any:
    """Safely casts value to specified type, returning default on failure."""
    if value is None:
        return default
    try:
        return cast_to(value)
    except (ValueError, TypeError):
        logging.warning(f"Could not cast '{value}' to {cast_to}. Using default: {default}")
        return default

def extract_text_from_segments(segments):
    logging.debug(f"Segments received: {segments}")
    logging.debug(f"Type of segments: {type(segments)}")

    text = ""

    if isinstance(segments, list):
        for segment in segments:
            logging.debug(f"Current segment: {segment}")
            logging.debug(f"Type of segment: {type(segment)}")
            if 'Text' in segment:
                text += segment['Text'] + " "
            else:
                logging.warning(f"Skipping segment due to missing 'Text' key: {segment}")
    else:
        logging.warning(f"Unexpected type of 'segments': {type(segments)}")

    return text.strip()


def get_openai_embeddings(input_data: str, model: str) -> List[float]:
    """
    Get embeddings for the input text from OpenAI API.

    Args:
        input_data (str): The input text to get embeddings for.
        model (str): The model to use for generating embeddings.

    Returns:
        List[float]: The embeddings generated by the API.
    """
    #FIXME
    api_key = "TEST_KEY"# loaded_config_data['openai_api']['api_key']

    if not api_key:
        logging.error("OpenAI: API key not found or is empty")
        raise ValueError("OpenAI: API Key Not Provided/Found in Config file or is empty")

    logging.debug(f"OpenAI: Using API Key: {api_key[:5]}...{api_key[-5:]}")
    logging.debug(f"OpenAI: Raw input data (first 500 chars): {str(input_data)[:500]}...")
    logging.debug(f"OpenAI: Using model: {model}")

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }

    request_data = {
        "input": input_data,
        "model": model,
    }

    try:
        logging.debug("OpenAI: Posting request to embeddings API")
        response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=request_data)
        logging.debug(f"Full API response data: {response}")
        if response.status_code == 200:
            response_data = response.json()
            if 'data' in response_data and len(response_data['data']) > 0:
                embedding = response_data['data'][0]['embedding']
                logging.debug("OpenAI: Embeddings retrieved successfully")
                return embedding
            else:
                logging.warning("OpenAI: Embedding data not found in the response")
                raise ValueError("OpenAI: Embedding data not available in the response")
        else:
            logging.error(f"OpenAI: Embeddings request failed with status code {response.status_code}")
            logging.error(f"OpenAI: Error response: {response.text}")
            raise ValueError(f"OpenAI: Failed to retrieve embeddings. Status code: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"OpenAI: Error making API request: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI: Error making API request: {str(e)}")
    except Exception as e:
        logging.error(f"OpenAI: Unexpected error: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI: Unexpected error occurred: {str(e)}")


def chat_with_openai(api_key, input_data, custom_prompt_arg, temp, system_message, streaming, maxp, model):
    provider_section_key = "openai" # Key used in [api_settings.*] in config.toml
    # https://platform.openai.com/docs/api-reference

    try:
        # --- Load Settings ---
        # API Key: Priority -> argument > env var (name from config) > config file key (fallback)
        openai_api_key = api_key # Prioritize argument
        if not openai_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "OPENAI_API_KEY")
            openai_api_key = os.environ.get(api_key_env_var_name)
            if openai_api_key:
                logging.info(f"OpenAI: Using API key from environment variable {api_key_env_var_name}")
            # else: # Optional: Fallback to reading directly from config (less secure)
            #     openai_api_key = get_setting("api_settings", f"{provider_section_key}.api_key")
            #     if openai_api_key:
            #         logging.warning("OpenAI: Using API key found directly in config file (less secure).")

        if not openai_api_key:
            logging.error("OpenAI: API key not found in argument, environment variable, or config.")
            return "OpenAI: API Key Not Provided/Found/Configured."
        logging.debug(f"OpenAI: Using API Key: {openai_api_key[:5]}...{openai_api_key[-5:]}")

        # Model: Priority -> argument > config default
        openai_model = model
        if not openai_model:
            openai_model = get_setting("api_settings", f"{provider_section_key}.model", "gpt-4o")
        logging.debug(f"OpenAI: Using model: {openai_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            # Handle string "true"/"false" from config if necessary
            if isinstance(streaming_cfg, str):
                streaming = streaming_cfg.lower() == "true"
            else:
                streaming = bool(streaming_cfg)
        else:
             streaming = bool(streaming) # Ensure boolean if passed as arg
        logging.debug(f"OpenAI: Streaming mode: {streaming}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7)
            temp = _safe_cast(temp_cfg, float, 0.7)
        else:
             temp = _safe_cast(temp, float, 0.7)
        logging.debug(f"OpenAI: Using temperature: {temp}")

        # Top_p (maxp): Priority -> argument > config default
        # Note: OpenAI uses 'top_p'. 'maxp' is likely the UI variable name.
        if maxp is None:
            maxp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 1.0)
            top_p_value = _safe_cast(maxp_cfg, float, 1.0)
        else:
             top_p_value = _safe_cast(maxp, float, 1.0)
        logging.debug(f"OpenAI: Using top_p: {top_p_value}")

        # Max Tokens: Load from config
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 4096)
        max_tokens = _safe_cast(max_tokens_cfg, int, 4096)
        logging.debug(f"OpenAI: Using max_tokens: {max_tokens}")

        # System Message: Priority -> argument > fixed default (or load from config if desired)
        if system_message is None:
            # You could load a default system prompt from config here too
            # system_message = get_setting("api_settings", f"{provider_section_key}.system_prompt", "Default prompt")
            system_message = "You are a helpful AI assistant."
        logging.debug(f"OpenAI: Using system message: {system_message[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 60)
        api_timeout = _safe_cast(timeout_cfg, int, 60)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, int, 5)  # Can be float too for backoff factor
        logging.debug(f"OpenAI: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Prepare Request ---
        headers = {
            'Authorization': f'Bearer {openai_api_key}',
            'Content-Type': 'application/json'
        }
        openai_prompt = f"{input_data}\n\n{custom_prompt_arg}" if custom_prompt_arg else input_data
        logging.debug(f"OpenAI: Combined prompt (first 500 chars): {openai_prompt[:500]}...")

        data = {
            "model": openai_model,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": openai_prompt}
            ],
            "max_tokens": max_tokens,  # Use max_tokens here
            "temperature": temp,
            "stream": streaming,
            "top_p": top_p_value  # Use the correct API parameter name
        }

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],  # Added 500
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)  # Only mount for https for OpenAI

        api_url = 'https://api.openai.com/v1/chat/completions'  # Standard URL

        if streaming:
            logging.debug(f"OpenAI: Posting streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                stream=True,
                timeout=api_timeout  # Use loaded timeout
            )
            # logging.debug(f"OpenAI: Raw Response Status: {response.status_code}") # Careful logging potentially sensitive data
            response.raise_for_status()  # Raise HTTP errors

            # --- Stream Processing Generator ---
            # (Keep the existing stream_generator function here)
            def stream_generator():
                # ... (existing generator code) ...
                pass  # Placeholder

            return stream_generator()
        else:
            logging.debug(f"OpenAI: Posting non-streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                timeout=api_timeout  # Use loaded timeout
            )
            logging.debug(f"OpenAI: Response Status: {response.status_code}")

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"OpenAI: Response Data: {response_data}") # Careful logging potentially large/sensitive data
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    chat_response = response_data['choices'][0]['message']['content'].strip()
                    logging.info("OpenAI: Chat request successful.")
                    # logging.debug(f"OpenAI: Chat response: {chat_response[:200]}...")
                    return chat_response
                else:
                    logging.warning("OpenAI: Chat response not found in the response data.")
                    return "OpenAI: Chat response format unexpected."
            else:
                logging.error(
                    f"OpenAI: Chat request failed. Status: {response.status_code}, Body: {response.text[:500]}...")  # Log snippet of error
                return f"OpenAI: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenAI: RequestException: {e}", exc_info=True)
        return f"OpenAI: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        logging.error(f"OpenAI: Error decoding JSON response: {e}", exc_info=True)
        return f"OpenAI: Error parsing API response."
    except Exception as e:
        logging.error(f"OpenAI: Unexpected error: {e}", exc_info=True)
        return f"OpenAI: Unexpected error occurred: {e}"


def chat_with_anthropic(api_key, input_data, model=None, custom_prompt_arg=None,
                        system_prompt=None, temp=None, streaming=None, topp=None, topk=None):
    """Interacts with the Anthropic API using settings from config."""
    provider_section_key = "anthropic" # Key used in [api_settings.*] in config.toml

    try:
        # --- Load Settings ---
        # API Key: Priority -> argument > env var (name from config) > config file key (fallback)
        anthropic_api_key = api_key # Prioritize argument
        if not anthropic_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "ANTHROPIC_API_KEY")
            anthropic_api_key = os.environ.get(api_key_env_var_name)
            if anthropic_api_key:
                logging.info(f"Anthropic: Using API key from environment variable {api_key_env_var_name}")
            # else: # Optional fallback (less secure)
            #     anthropic_api_key = get_setting("api_settings", f"{provider_section_key}.api_key")
            #     if anthropic_api_key: logging.warning("Anthropic: Using API key found directly in config file.")

        if not anthropic_api_key:
            logging.error("Anthropic: API key not found in argument, environment variable, or config.")
            return "Anthropic: API Key Not Provided/Found/Configured."
        logging.debug(f"Anthropic: Using API Key: {anthropic_api_key[:5]}...{anthropic_api_key[-5:]}")

        # Model: Priority -> argument > config default
        anthropic_model = model
        if not anthropic_model:
            anthropic_model = get_setting("api_settings", f"{provider_section_key}.model", "claude-3-haiku-20240307")
        logging.debug(f"Anthropic: Using model: {anthropic_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            streaming = _safe_cast(streaming_cfg, bool, False)
        else:
            streaming = bool(streaming) # Ensure boolean
        logging.debug(f"Anthropic: Streaming mode: {streaming}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7)
            temp = _safe_cast(temp_cfg, float, 0.7)
        else:
            temp = _safe_cast(temp, float, 0.7)
        logging.debug(f"Anthropic: Using temperature: {temp}")

        # Top_p (topp): Priority -> argument > config default
        if topp is None:
            topp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 1.0)
            top_p_value = _safe_cast(topp_cfg, float, 1.0)
        else:
            top_p_value = _safe_cast(topp, float, 1.0)
        logging.debug(f"Anthropic: Using top_p: {top_p_value}")

        # Top_k (topk): Priority -> argument > config default
        if topk is None:
            topk_cfg = get_setting("api_settings", f"{provider_section_key}.top_k", 0) # Default 0 often disables it
            top_k_value = _safe_cast(topk_cfg, int, 0)
        else:
            top_k_value = _safe_cast(topk, int, 0)
        logging.debug(f"Anthropic: Using top_k: {top_k_value}")

        # Max Tokens: Load from config
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 4096)
        max_tokens = _safe_cast(max_tokens_cfg, int, 4096)
        logging.debug(f"Anthropic: Using max_tokens: {max_tokens}")

        # System Message: Priority -> argument > default
        if system_prompt is None:
            # system_prompt = get_setting("api_settings", f"{provider_section_key}.system_prompt", "Default prompt") # Optional config load
            system_prompt = "You are a helpful assistant."
        logging.debug(f"Anthropic: Using system message: {system_prompt[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 90)
        api_timeout = _safe_cast(timeout_cfg, int, 90)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, float, 5.0) # Backoff factor can be float
        logging.debug(f"Anthropic: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Prepare Request ---
        headers = {
            'x-api-key': anthropic_api_key,
            'anthropic-version': '2023-06-01', # Keep appropriate version
            'Content-Type': 'application/json'
        }
        # Ensure input_data is a string before combining
        if isinstance(input_data, (list, dict)):
            # Attempt to extract text if it's segments, otherwise dump JSON
            if isinstance(input_data, list) and input_data and isinstance(input_data[0], dict) and 'Text' in input_data[0]:
                 base_text = extract_text_from_segments(input_data)
                 logging.debug("Anthropic: Extracted text from segments list.")
            else:
                 try:
                     base_text = json.dumps(input_data)
                     logging.debug("Anthropic: Converted non-segment list/dict input to JSON string.")
                 except TypeError:
                     base_text = str(input_data)
                     logging.warning(f"Anthropic: Could not JSON dump input_data of type {type(input_data)}, using str().")
        else:
             base_text = str(input_data) # Ensure string

        anthropic_combined_prompt = f"{base_text}\n\n{custom_prompt_arg}" if custom_prompt_arg else base_text
        logging.debug(f"Anthropic: Combined prompt (first 500 chars): {anthropic_combined_prompt[:500]}...")

        user_message = {"role": "user", "content": anthropic_combined_prompt}

        # Construct payload, conditionally adding top_k
        data = {
            "model": anthropic_model,
            "max_tokens": max_tokens,
            "messages": [user_message], # Anthropic API expects messages list
            # "stop_sequences": ["\n\nHuman:"], # Often useful, keep if needed
            "temperature": temp,
            "top_p": top_p_value,
            # "metadata": { "user_id": "example_user_id" }, # Optional metadata
            "stream": streaming,
            "system": system_prompt
        }
        # Only add top_k if it's meaningful (greater than 0)
        if top_k_value > 0:
            data["top_k"] = top_k_value
            logging.debug(f"Anthropic: Including top_k={top_k_value} in request.")

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504], # Anthropic can return 5xx
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)

        api_url = 'https://api.anthropic.com/v1/messages' # Anthropic messages endpoint

        if streaming:
            logging.debug(f"Anthropic: Posting streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                stream=True,
                timeout=api_timeout
            )
            # logging.debug(f"Anthropic: Raw Response Status: {response.status_code}")
            response.raise_for_status()

            # --- Stream Processing Generator ---
            def stream_generator():
                event_type = None # Track current SSE event type
                for line in response.iter_lines():
                    if not line: continue # Skip empty lines
                    line = line.decode('utf-8').strip()

                    if line.startswith('event:'):
                        event_type = line[len('event:'):].strip()
                        # logging.debug(f"Anthropic Stream Event: {event_type}")
                    elif line.startswith('data:'):
                        data_str = line[len('data:'):].strip()
                        # logging.debug(f"Anthropic Stream Data: {data_str}")
                        if data_str == '[DONE]': # Check for Anthropic's DONE marker if they use one (usually not in messages)
                            break
                        try:
                            data_json = json.loads(data_str)
                            # Process different event types based on Anthropic's Messages API streaming format
                            if event_type == 'message_start':
                                # Contains initial message metadata (role, model etc.)
                                logging.debug(f"Anthropic Stream: message_start received.")
                                pass # Usually no text content here
                            elif event_type == 'content_block_start':
                                # Indicates start of a text block
                                logging.debug(f"Anthropic Stream: content_block_start received.")
                                pass
                            elif event_type == 'content_block_delta':
                                # Contains actual text chunk
                                if data_json.get('type') == 'content_block_delta':
                                    delta = data_json.get('delta', {})
                                    if delta.get('type') == 'text_delta':
                                        text_chunk = delta.get('text', '')
                                        if text_chunk:
                                            yield text_chunk # Yield the text chunk
                            elif event_type == 'content_block_stop':
                                # Indicates end of a text block
                                logging.debug(f"Anthropic Stream: content_block_stop received.")
                                pass
                            elif event_type == 'message_delta':
                                # Contains changes to message metadata (like stop_reason)
                                logging.debug(f"Anthropic Stream: message_delta received.")
                                pass
                            elif event_type == 'message_stop':
                                # Final event indicating end of the message stream
                                logging.info(f"Anthropic Stream: message_stop received.")
                                break # End the generator
                            elif event_type == 'ping':
                                # Keep-alive event, ignore
                                pass
                            elif data_json.get('type') == 'error':
                                logging.error(f"Anthropic Stream: Error received: {data_json.get('error')}")
                                yield f"[ERROR: {data_json.get('error', {}).get('message', 'Unknown stream error')}]"
                                break # Stop on error
                            else:
                                logging.warning(f"Anthropic Stream: Unhandled event type '{event_type}' or data type '{data_json.get('type')}'")

                        except json.JSONDecodeError:
                            logging.error(f"Anthropic Stream: Error decoding JSON from line: {line}")
                            continue
                        except Exception as stream_err:
                             logging.error(f"Anthropic Stream: Error processing chunk: {stream_err}", exc_info=True)
                             yield f"[ERROR: {stream_err}]" # Yield error message
                             break
            return stream_generator()
        else:
            logging.debug(f"Anthropic: Posting non-streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                timeout=api_timeout
            )
            logging.debug(f"Anthropic: Response Status: {response.status_code}")
            response.raise_for_status() # Raise HTTP errors

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"Anthropic: Response Data: {response_data}")
                try:
                    # Extract the assistant's reply from the 'content' list
                    content_blocks = response_data.get('content', [])
                    full_response_text = ""
                    for block in content_blocks:
                        if block.get('type') == 'text':
                            full_response_text += block.get('text', '')
                    chat_response = full_response_text.strip()
                    if chat_response:
                         logging.info("Anthropic: Chat request successful.")
                         # logging.debug(f"Anthropic: Chat response: {chat_response[:200]}...")
                         return chat_response
                    else:
                         logging.warning("Anthropic: No text content found in response blocks.")
                         return "Anthropic: No text content found in response."
                except Exception as parse_err:
                    logging.error(f"Anthropic: Error parsing response content: {parse_err}", exc_info=True)
                    logging.debug(f"Anthropic: Raw response body: {response.text}")
                    return f"Anthropic: Error parsing response: {parse_err}"
            # No need for explicit 500 check, raise_for_status handles it if not retried
            else: # raise_for_status() handles non-200 after retries
                logging.error(f"Anthropic: Chat request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"Anthropic: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.HTTPError as e:
        # Handle errors raised by raise_for_status() after retries
        logging.error(f"Anthropic: HTTPError after retries: {e.response.status_code} - {e.response.text[:500]}...", exc_info=True)
        return f"Anthropic: API Error: {e.response.status_code}"
    except requests.exceptions.RequestException as e:
        logging.error(f"Anthropic: RequestException: {e}", exc_info=True)
        return f"Anthropic: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        # This might happen if parsing input_data fails if it's supposed to be JSON
        logging.error(f"Anthropic: Error decoding JSON (likely input or response): {e}", exc_info=True)
        return f"Anthropic: Error parsing JSON."
    except Exception as e:
        logging.error(f"Anthropic: Unexpected error in chat_with_anthropic: {e}", exc_info=True)
        return f"Anthropic: Unexpected error occurred: {e}"


# Summarize with Cohere
def chat_with_cohere(api_key=None, input_data=None, model=None, custom_prompt_arg=None, system_prompt=None, temp=None, streaming=None, topp=None, topk=None):
    """Interacts with the Cohere API using settings from config."""
    provider_section_key = "cohere" # Key used in [api_settings.*] in config.toml
    # Base API URL for Cohere chat
    cohere_api_url = 'https://api.cohere.com/v1/chat' # Use v1 for chat

    try:
        # --- Load Settings ---
        # API Key: Priority -> argument > env var (name from config) > config file key (fallback)
        cohere_api_key = api_key # Prioritize argument
        if not cohere_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "COHERE_API_KEY")
            cohere_api_key = os.environ.get(api_key_env_var_name)
            if cohere_api_key:
                logging.info(f"Cohere: Using API key from environment variable {api_key_env_var_name}")
            # else: # Optional: Fallback to reading directly from config (less secure)
            #     cohere_api_key = get_setting("api_settings", f"{provider_section_key}.api_key")
            #     if cohere_api_key:
            #         logging.warning("Cohere: Using API key found directly in config file (less secure).")

        if not cohere_api_key:
            logging.error("Cohere: API key not found in argument, environment variable, or config.")
            return "Cohere: API Key Not Provided/Found/Configured."
        logging.debug(f"Cohere: Using API Key: {cohere_api_key[:5]}...{cohere_api_key[-5:]}")

        # Model: Priority -> argument > config default
        cohere_model = model
        if not cohere_model:
            cohere_model = get_setting("api_settings", f"{provider_section_key}.model", "command-r-plus")
        logging.debug(f"Cohere: Using model: {cohere_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            streaming = _safe_cast(streaming_cfg, bool, False)
        else:
            streaming = _safe_cast(streaming, bool, False) # Ensure boolean if passed as arg
        logging.debug(f"Cohere: Streaming mode: {streaming}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.3)
            temp = _safe_cast(temp_cfg, float, 0.3)
        else:
             temp = _safe_cast(temp, float, 0.3)
        logging.debug(f"Cohere: Using temperature: {temp}")

        # Top-P (topp): Priority -> argument > config default
        # Cohere uses 'p'. 'topp' is likely the UI variable name.
        if topp is None:
            topp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 0.75)
            p_value = _safe_cast(topp_cfg, float, 0.75)
        else:
             p_value = _safe_cast(topp, float, 0.75)
        logging.debug(f"Cohere: Using p (top_p): {p_value}")

        # Top-K (topk): Priority -> argument > config default
        # Cohere uses 'k'.
        if topk is None:
            topk_cfg = get_setting("api_settings", f"{provider_section_key}.top_k", 0) # Default 0 often disables it
            k_value = _safe_cast(topk_cfg, int, 0)
        else:
             k_value = _safe_cast(topk, int, 0)
        logging.debug(f"Cohere: Using k (top_k): {k_value}")

        # Max Tokens: Load from config
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 4096)
        max_tokens = _safe_cast(max_tokens_cfg, int, 4096)
        logging.debug(f"Cohere: Using max_tokens: {max_tokens}")

        # System Message: Priority -> argument > fixed default (or load from config)
        if system_prompt is None:
            # system_prompt = get_setting("api_settings", f"{provider_section_key}.system_prompt", "Default prompt")
            system_prompt = "You are a helpful assistant."
        logging.debug(f"Cohere: Using system message (preamble): {system_prompt[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 90)
        api_timeout = _safe_cast(timeout_cfg, int, 90)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, int, 5) # Or float
        logging.debug(f"Cohere: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Prepare Request ---
        headers = {
            'accept': 'application/json',
            'content-type': 'application/json',
            'Authorization': f'Bearer {cohere_api_key}',
            'Cohere-Version': '2022-12-06' # Specify Cohere API version
        }

        # Cohere uses 'message' for the user input and 'preamble' for the system message
        user_message = f"{input_data}\n\n{custom_prompt_arg}" if custom_prompt_arg else input_data
        logging.debug(f"Cohere: User message (first 500 chars): {user_message[:500]}...")

        data = {
            "model": cohere_model,
            "message": user_message,
            "preamble": system_prompt, # Use 'preamble' for system message
            "temperature": temp,
            "p": p_value,       # Use Cohere's parameter 'p'
            "k": k_value,       # Use Cohere's parameter 'k'
            "max_tokens": max_tokens,
            "stream": streaming # Pass the streaming flag
            # Add other parameters like 'connectors' if needed
        }
        # Remove None values? Cohere might handle them, check docs. Let's keep them for now.
        # data = {k: v for k, v in data.items() if v is not None}

        logging.debug(f"Cohere: Request data: {json.dumps(data, indent=2, default=str)}") # Use default=str for safety

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter) # Mount only for https

        if streaming:
            logging.debug(f"Cohere: Posting streaming request to {cohere_api_url}")
            response = session.post(
                cohere_api_url,
                headers=headers,
                json=data,
                stream=True,
                timeout=api_timeout
            )
            # logging.debug(f"Cohere: Raw Response Status: {response.status_code}")
            response.raise_for_status()

            # --- Stream Processing ---
            # Cohere streaming format is different (JSON objects per line)
            def stream_generator():
                buffer = ""
                for chunk in response.iter_content(chunk_size=None): # Iterate over raw bytes
                    buffer += chunk.decode('utf-8')
                    while '\n' in buffer:
                        line, buffer = buffer.split('\n', 1)
                        line = line.strip()
                        if not line: continue
                        try:
                            event_data = json.loads(line)
                            event_type = event_data.get('event_type')
                            logging.debug(f"Cohere Stream Event: {event_type}, Data: {event_data}") # Log event type

                            if event_type == 'text-generation':
                                yield event_data.get('text', '')
                            elif event_type == 'stream-end':
                                logging.info("Cohere: Stream end event received.")
                                # You might want to yield final response info here if needed
                                # final_response = event_data.get('response')
                                # if final_response: yield f"\n[STREAM END INFO: {final_response.get('id')}]"
                                return # End the generator cleanly
                            # Handle other event types like 'citations' if needed
                            # elif event_type == 'citation-generation':
                            #    logging.debug(f"Cohere citation: {event_data.get('citations')}")
                            # elif event_type == 'tool-calls-generation':
                            #    logging.debug(f"Cohere tool calls: {event_data.get('tool_calls')}")

                        except json.JSONDecodeError:
                            logging.error(f"Cohere: Error decoding JSON from stream line: {line}")
                            continue
                        except Exception as stream_e:
                             logging.error(f"Cohere: Error processing stream event: {stream_e} - Line: {line}", exc_info=True)
                # Handle any remaining buffer content if needed (usually shouldn't happen with newline splitting)
                if buffer.strip():
                     logging.warning(f"Cohere: Remaining stream buffer content: {buffer}")


            return stream_generator()

        else: # Non-streaming
            logging.debug(f"Cohere: Posting non-streaming request to {cohere_api_url}")
            response = session.post(
                cohere_api_url,
                headers=headers,
                json=data,
                timeout=api_timeout
            )
            logging.debug(f"Cohere: Response Status: {response.status_code}")

            if response.status_code == 200:
                try:
                    response_data = response.json()
                    # logging.debug(f"Cohere: Response Data: {response_data}")
                except json.JSONDecodeError as e:
                    logging.error(f"Cohere: Failed to decode JSON response: {e}", exc_info=True)
                    return "Cohere: Failed to decode JSON response"

                # Extract text from Cohere's non-streaming response format
                chat_response = response_data.get('text', '').strip()
                if chat_response:
                    logging.info("Cohere: Chat request successful.")
                    return chat_response
                else:
                    # Check for potential errors within the response body
                    if 'message' in response_data: # Cohere often includes error messages here
                         logging.error(f"Cohere: API returned an error message: {response_data['message']}")
                         return f"Cohere: API Error - {response_data['message']}"
                    else:
                         logging.warning(f"Cohere: 'text' field not found or empty in response: {response_data}")
                         return "Cohere: Response format unexpected (missing text)."

            elif response.status_code == 401:
                logging.error("Cohere: Unauthorized (401). Check API key.")
                return "Cohere: Unauthorized - Invalid API key"
            else:
                logging.error(f"Cohere: Request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"Cohere: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e:
        logging.error(f"Cohere: RequestException: {e}", exc_info=True)
        return f"Cohere: Network or Request Error: {e}"
    except Exception as e:
        logging.error(f"Cohere: Unexpected error: {e}", exc_info=True)
        return f"Cohere: Unexpected error occurred: {e}"


# https://console.groq.com/docs/quickstart
def chat_with_groq(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=None, maxp=None, model=None):
    """Interacts with the Groq API using settings from config."""
    provider_section_key = "groq" # Key used in [api_settings.*] in config.toml
    logging.debug(f"Groq: Chat process starting for model '{model or 'default'}'...")

    try:
        # --- Load Settings ---
        # API Key: Priority -> argument > env var (name from config) > config file key (fallback)
        groq_api_key = api_key # Prioritize argument
        if not groq_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "GROQ_API_KEY")
            groq_api_key = os.environ.get(api_key_env_var_name)
            if groq_api_key:
                logging.info(f"Groq: Using API key from environment variable {api_key_env_var_name}")
            # else: # Optional fallback (less secure)
            #     groq_api_key = get_setting("api_settings", f"{provider_section_key}.api_key")
            #     if groq_api_key: logging.warning("Groq: Using API key found directly in config file.")

        if not groq_api_key:
            logging.error("Groq: API key not found in argument, environment variable, or config.")
            return "Groq: API Key Not Provided/Found/Configured."
        logging.debug(f"Groq: Using API Key: {groq_api_key[:5]}...{groq_api_key[-5:]}")

        # Model: Priority -> argument > config default
        groq_model = model # Prioritize argument
        if not groq_model:
            groq_model = get_setting("api_settings", f"{provider_section_key}.model", "llama3-70b-8192")
        logging.debug(f"Groq: Using model: {groq_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            streaming = str(streaming_cfg).lower() == "true" if isinstance(streaming_cfg, str) else bool(streaming_cfg)
        else:
             streaming = bool(streaming)
        logging.debug(f"Groq: Streaming mode: {streaming}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7)
            temp = _safe_cast(temp_cfg, float, 0.7)
        else:
             temp = _safe_cast(temp, float, 0.7)
        logging.debug(f"Groq: Using temperature: {temp}")

        # Top_p (maxp): Priority -> argument > config default
        # Groq API uses 'top_p'. 'maxp' comes from the UI.
        if maxp is None:
            maxp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 1.0)
            top_p_value = _safe_cast(maxp_cfg, float, 1.0)
        else:
             top_p_value = _safe_cast(maxp, float, 1.0)
        logging.debug(f"Groq: Using top_p: {top_p_value}")

        # Max Tokens: Load from config
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 8192)
        max_tokens = _safe_cast(max_tokens_cfg, int, 8192)
        logging.debug(f"Groq: Using max_tokens: {max_tokens}")

        # System Message: Priority -> argument > fixed default
        if system_message is None:
            system_message = "You are a helpful AI assistant." # Or load from config if needed
        logging.debug(f"Groq: Using system message: {system_message[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 60)
        api_timeout = _safe_cast(timeout_cfg, int, 60)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, (int, float), 5) # Allow float for backoff
        logging.debug(f"Groq: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Input Data Handling (Keep existing logic) ---
        if isinstance(input_data, str) and os.path.isfile(input_data):
            logging.debug("Groq: Loading json data from file path")
            try:
                with open(input_data, 'r', encoding='utf-8') as file: # Specify encoding
                    extracted_data = json.load(file)
            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                 logging.error(f"Groq: Failed to read/decode JSON from {input_data}: {e}. Treating as plain text.")
                 with open(input_data, 'r', encoding='utf-8', errors='replace') as file:
                     extracted_data = file.read() # Fallback to reading as text
            except FileNotFoundError:
                 logging.error(f"Groq: Input file not found: {input_data}")
                 return f"Groq: Input file not found: {input_data}"
        else:
            logging.debug("Groq: Using provided string/data directly")
            extracted_data = input_data

        # Extract text based on type
        if isinstance(extracted_data, list):
            text = extract_text_from_segments(extracted_data) # Assuming extract_text_from_segments handles lists
        elif isinstance(extracted_data, dict) and 'segments' in extracted_data: # Check for common structure
             text = extract_text_from_segments(extracted_data['segments'])
        elif isinstance(extracted_data, str):
            text = extracted_data
        else:
            logging.warning(f"Groq: Unhandled input data type: {type(extracted_data)}. Attempting string conversion.")
            text = str(extracted_data) # Attempt conversion as fallback

        logging.debug(f"Groq: Extracted text (first 500 chars): {text[:500]}...")


        # --- Prepare Request ---
        headers = {
            'Authorization': f'Bearer {groq_api_key}',
            'Content-Type': 'application/json'
        }
        # Combine input text and custom prompt
        groq_prompt = f"{text}\n\n{custom_prompt_arg}" if custom_prompt_arg else text
        logging.debug(f"Groq: Combined prompt (first 500 chars): {groq_prompt[:500]}...")

        # Build API payload using loaded/validated variables and correct API names
        data = {
            "model": groq_model,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": groq_prompt}
            ],
            "temperature": temp,
            "stream": streaming,
            "top_p": top_p_value, # Groq uses top_p
            "max_tokens": max_tokens
        }

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter) # Groq API is HTTPS

        api_url = 'https://api.groq.com/openai/v1/chat/completions' # Standard Groq URL

        if streaming:
            logging.debug(f"Groq: Posting streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                stream=True,
                timeout=api_timeout
            )
            response.raise_for_status()

            # --- Stream Processing Generator ---
            def stream_generator():
                try:
                    for line in response.iter_lines():
                        if line:
                            decoded_line = line.decode("utf-8").strip()
                            if decoded_line == "": continue
                            if decoded_line.startswith("data: "):
                                data_str = decoded_line[len("data: "):]
                                if data_str == "[DONE]": break
                                try:
                                    data_json = json.loads(data_str)
                                    chunk = data_json["choices"][0]["delta"].get("content", "")
                                    yield chunk
                                except json.JSONDecodeError:
                                    logging.error(f"Groq: Error decoding stream JSON: {decoded_line}")
                                except (KeyError, IndexError) as e:
                                     logging.error(f"Groq: Error parsing stream structure ({e}): {data_str}")
                                     continue # Skip malformed chunk
                finally:
                     response.close() # Ensure connection is closed
                     logging.debug("Groq: Streaming response closed.")

            return stream_generator()

        else: # Non-streaming
            logging.debug(f"Groq: Posting non-streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                timeout=api_timeout
            )
            logging.debug(f"Groq: Response Status: {response.status_code}")

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"Groq: Response Data: {response_data}")
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    chat_response = response_data['choices'][0]['message']['content'].strip()
                    logging.info("Groq: Chat request successful.")
                    # logging.debug(f"Groq: Chat response: {chat_response[:200]}...")
                    return chat_response
                else:
                    logging.warning("Groq: Chat response not found in the response data.")
                    return "Groq: Chat response format unexpected."
            else:
                logging.error(f"Groq: Chat request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"Groq: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e:
        logging.error(f"Groq: RequestException: {e}", exc_info=True)
        return f"Groq: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        logging.error(f"Groq: Error decoding JSON response: {e}", exc_info=True)
        return f"Groq: Error parsing API response."
    except ValueError as e: # Catch specific ValueErrors like format issues
         logging.error(f"Groq: Value error processing input: {e}", exc_info=True)
         return f"Groq: Invalid input data: {e}"
    except Exception as e:
        logging.error(f"Groq: Unexpected error: {e}", exc_info=True)
        return f"Groq: Unexpected error occurred: {e}"


def chat_with_openrouter(api_key=None, input_data=None, custom_prompt_arg=None, temp=None, system_message=None, streaming=None, top_p=None, top_k=None, minp=None, model=None): # Added model argument
    """Interacts with the OpenRouter API using settings from config."""
    provider_section_key = "openrouter" # Key used in [api_settings.*] in config.toml
    openrouter_api_url = "https://openrouter.ai/api/v1/chat/completions"

    try:
        logging.info(f"OpenRouter: Initiating chat request.")

        # --- Load Settings ---
        # API Key: Priority -> argument > env var > config fallback
        openrouter_api_key = api_key
        if not openrouter_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "OPENROUTER_API_KEY")
            openrouter_api_key = os.environ.get(api_key_env_var_name)
            if openrouter_api_key:
                logging.info(f"OpenRouter: Using API key from environment variable {api_key_env_var_name}")
            # else: # Optional fallback (less secure)
            #     openrouter_api_key = get_setting("api_settings", f"{provider_section_key}.api_key")
            #     if openrouter_api_key: logging.warning("OpenRouter: Using API key from config file.")

        if not openrouter_api_key:
            logging.error("OpenRouter: API key not found.")
            return "OpenRouter: API Key Not Provided/Found/Configured."
        logging.debug(f"OpenRouter: Using API Key: {openrouter_api_key[:5]}...{openrouter_api_key[-5:]}")

        # Model: Priority -> argument > config default
        openrouter_model = model
        if not openrouter_model:
            openrouter_model = get_setting("api_settings", f"{provider_section_key}.model", "meta-llama/Llama-3.1-8B-Instruct")
        logging.debug(f"OpenRouter: Using model: {openrouter_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            streaming = _safe_cast(streaming_cfg, bool, False)
        else:
            streaming = bool(streaming)
        logging.debug(f"OpenRouter: Streaming mode: {streaming}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7)
            temp = _safe_cast(temp_cfg, float, 0.7)
        else:
            temp = _safe_cast(temp, float, 0.7)
        logging.debug(f"OpenRouter: Using temperature: {temp}")

        # Top_p: Priority -> argument > config default
        if top_p is None:
            top_p_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 1.0)
            top_p = _safe_cast(top_p_cfg, float, 1.0)
        else:
            top_p = _safe_cast(top_p, float, 1.0)
        logging.debug(f"OpenRouter: Using top_p: {top_p}")

        # Top_k: Priority -> argument > config default
        if top_k is None:
            top_k_cfg = get_setting("api_settings", f"{provider_section_key}.top_k", 0) # OpenRouter defaults to 0 (disabled)
            top_k = _safe_cast(top_k_cfg, int, 0)
        else:
            top_k = _safe_cast(top_k, int, 0)
        logging.debug(f"OpenRouter: Using top_k: {top_k}")

        # Min_p: Priority -> argument > config default
        if minp is None:
            minp_cfg = get_setting("api_settings", f"{provider_section_key}.min_p", 0.0)
            minp = _safe_cast(minp_cfg, float, 0.0)
        else:
            minp = _safe_cast(minp, float, 0.0)
        logging.debug(f"OpenRouter: Using min_p: {minp}")

        # Max Tokens: Load from config
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 4096)
        max_tokens = _safe_cast(max_tokens_cfg, int, 4096)
        logging.debug(f"OpenRouter: Using max_tokens: {max_tokens}")

        # System Message: Priority -> argument > fixed default
        if system_message is None:
            system_message = "You are a helpful AI assistant."
        logging.debug(f"OpenRouter: Using system message: {system_message[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 120)
        api_timeout = _safe_cast(timeout_cfg, int, 120)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, float, 5.0) # Use float for backoff factor
        logging.debug(f"OpenRouter: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Prepare Request ---
        headers = {
            "Authorization": f"Bearer {openrouter_api_key}",
            "Content-Type": "application/json",
            # OpenRouter recommends adding HTTP Referer and X-Title
            "HTTP-Referer": "http://localhost:8000", # Replace with your actual app URL/name
            "X-Title": "TLDW-CLI", # Replace with your app name
        }
        # Accept header based on streaming
        headers["Accept"] = "text/event-stream" if streaming else "application/json"

        # Combine input data and prompt
        # Check if input_data is already text or needs extraction
        if isinstance(input_data, list):
             text_input = extract_text_from_segments(input_data) # Assuming this handles lists
        elif isinstance(input_data, str):
             text_input = input_data
        else:
             logging.error(f"OpenRouter: Invalid input_data type: {type(input_data)}")
             return "OpenRouter: Invalid input data format"

        combined_prompt = f"{text_input}\n\n{custom_prompt_arg}" if custom_prompt_arg else text_input
        logging.debug(f"OpenRouter: Combined prompt (first 500 chars): {combined_prompt[:500]}...")

        payload = {
            "model": openrouter_model,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": combined_prompt}
            ],
            "temperature": temp,
            "top_p": top_p,
            "top_k": top_k,
            "min_p": minp,
            "max_tokens": max_tokens, # Include max_tokens
            "stream": streaming
        }

        # Remove None values or default values that might interfere (e.g., top_k=0)
        # OpenRouter generally handles defaults well, but this can be safer
        # payload = {k: v for k, v in payload.items() if v is not None}
        # if 'top_k' in payload and payload['top_k'] == 0: del payload['top_k'] # Example: remove if 0 disables

        logging.debug(f"OpenRouter: Payload: {json.dumps({k: v for k, v in payload.items() if k != 'messages'}, indent=2)}") # Log payload without messages

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"] # Important: Retry POST requests
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)

        if streaming:
            logging.debug(f"OpenRouter: Posting streaming request to {openrouter_api_url}")
            response = session.post(
                url=openrouter_api_url,
                headers=headers,
                json=payload, # Use json parameter for auto-serialization
                stream=True,
                timeout=api_timeout
            )
            # logging.debug(f"OpenRouter: Raw Response Status: {response.status_code}")
            response.raise_for_status()

            # --- Stream Processing Generator ---
            def stream_generator():
                buffer = ""
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8')
                        buffer += decoded_line + "\n" # Append line to buffer

                        # Process buffer line by line (OpenRouter SSE might have multiple events per chunk)
                        while '\n\n' in buffer:
                            event_data, buffer = buffer.split('\n\n', 1)
                            if event_data.strip() == "event: data": continue # Ignore event type line if present
                            if event_data.startswith('data: '):
                                json_str = event_data[len('data: '):].strip()
                                if json_str == '[DONE]':
                                    logging.debug("OpenRouter: [DONE] signal received.")
                                    yield "[DONE]" # Signal completion if needed upstream
                                    return # End generation
                                try:
                                    data_json = json.loads(json_str)
                                    if 'choices' in data_json and len(data_json['choices']) > 0:
                                        delta = data_json['choices'][0].get('delta', {})
                                        content = delta.get('content') # Content can be None or empty string
                                        if content is not None: # Yield even empty strings if intended by API
                                            yield content
                                except json.JSONDecodeError as e:
                                    logging.error(f"OpenRouter: Error decoding streaming JSON chunk: {e} - Line: '{json_str}'")
                                    continue
                            else:
                                 logging.warning(f"OpenRouter: Received non-data line in stream: {event_data}")
            return stream_generator()

        else: # Non-streaming
            logging.debug(f"OpenRouter: Posting non-streaming request to {openrouter_api_url}")
            response = session.post(
                url=openrouter_api_url,
                headers=headers,
                json=payload, # Use json parameter
                timeout=api_timeout
            )
            logging.debug(f"OpenRouter: Response Status: {response.status_code}")

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"OpenRouter: Response Data: {response_data}")
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    chat_response = response_data['choices'][0]['message']['content'].strip()
                    logging.info("OpenRouter: Chat request successful.")
                    # logging.debug(f"OpenRouter: Chat response: {chat_response[:200]}...")
                    return chat_response
                else:
                    logging.warning("OpenRouter: Chat response not found in expected format.")
                    logging.debug(f"Unexpected OpenRouter response format: {response_data}")
                    return "OpenRouter: Chat response format unexpected."
            else:
                logging.error(f"OpenRouter: Chat request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"OpenRouter: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenRouter: RequestException: {e}", exc_info=True)
        return f"OpenRouter: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        logging.error(f"OpenRouter: Error decoding JSON response: {e}", exc_info=True)
        return f"OpenRouter: Error parsing API response."
    except ValueError as e: # Catch specific ValueErrors like invalid bool strings
        logging.error(f"OpenRouter: Value Error (likely config or casting): {e}", exc_info=True)
        return f"OpenRouter: Configuration or Input Error: {e}"
    except Exception as e:
        logging.error(f"OpenRouter: Unexpected error: {e}", exc_info=True)
        return f"OpenRouter: Unexpected error occurred: {e}"


def chat_with_huggingface(
    api_key: Optional[str],
    input_data: str,
    custom_prompt_arg: Optional[str],
    model: Optional[str] = None, # Added model parameter
    system_prompt: Optional[str] = None,
    temp: Optional[float] = None,
    streaming: Optional[bool] = None,
    topp: Optional[float] = None, # Added top_p parameter
    topk: Optional[int] = None   # Added top_k parameter
) -> Union[str, Generator[Any, Any, None], None]:
    """
    Interacts with the Hugging Face Inference API (Chat Completions endpoint)
    using settings from config.
    """
    provider_section_key = "huggingface" # Key in config.toml [api_settings.*]
    logging.debug(f"HuggingFace Chat: Process starting...")

    try:
        # --- Load Settings ---
        # API Key
        huggingface_api_key = api_key # Prioritize argument
        if not huggingface_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "HUGGINGFACE_API_KEY")
            huggingface_api_key = os.environ.get(api_key_env_var_name)
            if huggingface_api_key:
                logging.info(f"HuggingFace: Using API key from environment variable {api_key_env_var_name}")

        if not huggingface_api_key:
            logging.error("HuggingFace: API key not found in argument or environment variable.")
            return "HuggingFace: API Key Not Provided/Configured."
        logging.debug(f"HuggingFace: Using API Key: {huggingface_api_key[:5]}...{huggingface_api_key[-5:]}")

        # Model
        huggingface_model = model # Prioritize argument
        if not huggingface_model:
            huggingface_model = get_setting("api_settings", f"{provider_section_key}.model", "mistralai/Mixtral-8x7B-Instruct-v0.1")
        logging.debug(f"HuggingFace: Using model: {huggingface_model}")

        # API URL (Derived from model)
        # Use the standard HF Inference API endpoint for chat completions
        api_url = f"https://api-inference.huggingface.co/models/{huggingface_model}"
        # The specific task path might be added later or implied by the library/endpoint
        chat_api_url = f"{api_url}/v1/chat/completions" # Assuming OpenAI compatibility
        logging.debug(f"HuggingFace: Target API URL: {chat_api_url}")

        # Streaming
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            streaming = streaming_cfg.lower() == "true" if isinstance(streaming_cfg, str) else bool(streaming_cfg)
        else:
            streaming = bool(streaming) # Ensure boolean
        logging.debug(f"HuggingFace: Streaming mode: {streaming}")

        # Temperature
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7)
            temp = _safe_cast(temp_cfg, float, 0.7)
        else:
             temp = _safe_cast(temp, float, 0.7)
        logging.debug(f"HuggingFace: Using temperature: {temp}")

        # Top-P
        if topp is None:
            topp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 1.0)
            top_p_value = _safe_cast(topp_cfg, float, 1.0)
        else:
            top_p_value = _safe_cast(topp, float, 1.0)
        logging.debug(f"HuggingFace: Using top_p: {top_p_value}")

        # Top-K
        if topk is None:
            topk_cfg = get_setting("api_settings", f"{provider_section_key}.top_k", 50)
            top_k_value = _safe_cast(topk_cfg, int, 50)
        else:
            top_k_value = _safe_cast(topk, int, 50)
        # Handle disabling top_k (HF API might expect null or specific value like 0/-1)
        if top_k_value <= 0:
            top_k_value = None # Often None disables it, check HF docs if issues arise
            logging.debug("HuggingFace: Disabling top_k (set to None).")
        else:
             logging.debug(f"HuggingFace: Using top_k: {top_k_value}")

        # Max Tokens
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 4096)
        max_tokens_value = _safe_cast(max_tokens_cfg, int, 4096)
        logging.debug(f"HuggingFace: Using max_tokens: {max_tokens_value}")

        # System Prompt (Check if HF Chat endpoint uses it)
        # The OpenAI compatible endpoint SHOULD use it.
        if system_prompt is None:
            system_prompt = "You are a helpful assistant." # Simple default
        logging.debug(f"HuggingFace: Using system message: {system_prompt[:100]}...")

        # Timeout, Retries, Delay
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 60)
        api_timeout = _safe_cast(timeout_cfg, int, 60)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, int, 5)
        logging.debug(f"HuggingFace: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Prepare Request ---
        headers = {
            "Authorization": f"Bearer {huggingface_api_key}",
            'Content-Type': 'application/json' # Needed for OpenAI compatible endpoint
        }
        # Combine input data and custom prompt for the user message
        user_content = f"{input_data}"
        if custom_prompt_arg:
            user_content += f"\n\n{custom_prompt_arg}"
        logging.debug(f"HuggingFace: User content (first 500 chars): {user_content[:500]}...")

        messages = []
        if system_prompt:
             messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_content})

        # Construct payload, remove None values for optional params
        data = {
            "model": huggingface_model, # Pass model name
            "messages": messages,
            "max_tokens": max_tokens_value,
            "stream": streaming,
            "temperature": temp,
            "top_p": top_p_value,
            # Add top_k only if it has a value (is not None)
            **({"top_k": top_k_value} if top_k_value is not None else {}),
        }
        # Filter out None values from the payload before sending
        payload = {k: v for k, v in data.items() if v is not None}
        logging.debug(f"HuggingFace: Payload: {payload}")


        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504], # Retriable status codes
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter) # Mount only for https

        if streaming:
            logging.debug(f"HuggingFace: Posting streaming request to {chat_api_url}")
            response = session.post(
                chat_api_url,
                headers=headers,
                json=payload, # Send filtered payload
                stream=True,
                timeout=api_timeout
            )
            logging.debug(f"HuggingFace: Response Status: {response.status_code}")
            response.raise_for_status() # Raise HTTP errors

            # --- Stream Processing ---
            def stream_generator() -> Generator[str, None, None]:
                try:
                    for line in response.iter_lines():
                        if line:
                            decoded_line = line.decode('utf-8').strip()
                            if decoded_line.startswith('data:'):
                                data_str = decoded_line[len('data:'):].strip()
                                if data_str == '[DONE]':
                                    logging.debug("HuggingFace: Received [DONE] marker.")
                                    break
                                try:
                                    data_json = json.loads(data_str)
                                    # Standard OpenAI stream format check
                                    if 'choices' in data_json and len(data_json['choices']) > 0:
                                        delta = data_json['choices'][0].get('delta', {})
                                        chunk = delta.get('content') # Content is the text chunk
                                        if chunk: # Only yield if content exists
                                             yield chunk
                                    # Fallback for older/different HF stream formats (less likely with /v1/chat)
                                    # elif 'token' in data_json and 'text' in data_json['token']:
                                    #      yield data_json['token']['text']
                                    # else:
                                    #      logging.warning(f"HuggingFace: Unhandled stream chunk format: {data_json}")
                                except json.JSONDecodeError:
                                    logging.error(f"HuggingFace: Error decoding JSON from stream line: {decoded_line}")
                                except KeyError:
                                     logging.error(f"HuggingFace: Missing expected key in stream data: {data_json}")
                finally:
                     response.close() # Ensure response is closed
                     logging.info("HuggingFace: Streaming finished.")

            return stream_generator() # Return the generator

        else: # Non-streaming
            logging.debug(f"HuggingFace: Posting non-streaming request to {chat_api_url}")
            response = session.post(
                chat_api_url,
                headers=headers,
                json=payload, # Send filtered payload
                timeout=api_timeout
            )
            logging.debug(f"HuggingFace: Response Status: {response.status_code}")

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"HuggingFace: Response Data: {response_data}")
                # Check according to OpenAI compatible structure
                if "choices" in response_data and len(response_data["choices"]) > 0:
                    message_content = response_data["choices"][0].get("message", {}).get("content")
                    if message_content:
                        logging.info("HuggingFace Chat: Chat request successful.")
                        return message_content.strip()
                    else:
                         logging.error("HuggingFace Chat: 'content' missing in response choices.")
                         return "HuggingFace Chat: Response format missing content."
                else:
                    logging.error("HuggingFace Chat: 'choices' missing or empty in response.")
                    # Check for older 'generated_text' format as a fallback?
                    if "generated_text" in response_data:
                         logging.warning("HuggingFace: Falling back to 'generated_text' key.")
                         return response_data["generated_text"].strip()
                    return "HuggingFace Chat: Response format error (no choices)."
            else:
                logging.error(f"HuggingFace Chat: Request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"HuggingFace Chat: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e:
        logging.error(f"HuggingFace: RequestException: {e}", exc_info=True)
        return f"HuggingFace: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        # This might happen if the response is not JSON (e.g., HTML error page)
        logging.error(f"HuggingFace: Error decoding JSON response: {e}. Response text: {response.text[:500] if 'response' in locals() else 'N/A'}", exc_info=True)
        return f"HuggingFace: Error parsing API response."
    except Exception as e:
        logging.error(f"HuggingFace: Unexpected error: {e}", exc_info=True)
        return f"HuggingFace: Unexpected error occurred: {e}"


def chat_with_deepseek(api_key, input_data, custom_prompt_arg, model=None, temp=None, system_message=None, streaming=None, topp=None):
    """
    Interacts with the DeepSeek API using settings from config.

    Args:
        api_key (Optional[str]): API key passed directly.
        input_data (Union[str, list, dict]): Input text, path, or segments.
        custom_prompt_arg (Optional[str]): Additional prompt text.
        model (Optional[str]): Specific model to use.
        temp (Optional[float]): Temperature override.
        system_message (Optional[str]): System message override.
        streaming (Optional[bool]): Streaming override.
        topp (Optional[float]): Top-P override.

    Returns:
        Union[str, Generator[str, None, None]]: Response string or stream generator.
    """
    logging.info("DeepSeek: Chat request process starting...")
    provider_section_key = "deepseek" # Key in [api_settings.*] in config.toml

    try:
        # --- Load Settings ---
        # API Key: Priority -> argument > env var > config file key (fallback)
        deepseek_api_key = api_key
        if not deepseek_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "DEEPSEEK_API_KEY")
            deepseek_api_key = os.environ.get(api_key_env_var_name)
            if deepseek_api_key:
                logging.info(f"DeepSeek: Using API key from environment variable {api_key_env_var_name}")
            # else: # Optional fallback to config file (less secure)
                # deepseek_api_key = get_setting("api_settings", f"{provider_section_key}.api_key")
                # if deepseek_api_key: logging.warning("DeepSeek: Using API key from config file.")

        if not deepseek_api_key:
            logging.error("DeepSeek: API key not found in argument, environment, or config.")
            return "DeepSeek: API Key Not Provided/Found/Configured."
        logging.debug(f"DeepSeek: Using API Key: {deepseek_api_key[:5]}...{deepseek_api_key[-5:]}")

        # Model: Priority -> argument > config default
        deepseek_model = model
        if not deepseek_model:
            deepseek_model = get_setting("api_settings", f"{provider_section_key}.model", "deepseek-chat")
        logging.debug(f"DeepSeek: Using model: {deepseek_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            streaming_value = str(streaming_cfg).lower() == "true" if isinstance(streaming_cfg, str) else bool(streaming_cfg)
        else:
             streaming_value = bool(streaming)
        logging.debug(f"DeepSeek: Streaming mode: {streaming_value}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7) # Default 0.7 now
            temp_value = _safe_cast(temp_cfg, float, 0.7)
        else:
             temp_value = _safe_cast(temp, float, 0.7)
        logging.debug(f"DeepSeek: Using temperature: {temp_value}")

        # Top_p (topp): Priority -> argument > config default
        if topp is None:
            topp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 1.0)
            top_p_value = _safe_cast(topp_cfg, float, 1.0)
        else:
             top_p_value = _safe_cast(topp, float, 1.0)
        logging.debug(f"DeepSeek: Using top_p: {top_p_value}")

        # Max Tokens: Load from config
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 4096)
        max_tokens_value = _safe_cast(max_tokens_cfg, int, 4096)
        logging.debug(f"DeepSeek: Using max_tokens: {max_tokens_value}")

        # System Message: Priority -> argument > fixed default
        deepseek_system_message = system_message
        if deepseek_system_message is None:
            deepseek_system_message = "You are a helpful AI assistant." # Default
        logging.debug(f"DeepSeek: Using system message: {deepseek_system_message[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 60)
        api_timeout = _safe_cast(timeout_cfg, int, 60)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, int, 5) # Use int or float based on Retry backoff_factor type
        logging.debug(f"DeepSeek: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Input Data Handling ---
        # (Keep existing logic to handle file paths, JSON, strings, lists)
        processed_data = None
        if isinstance(input_data, str) and os.path.isfile(input_data):
            logging.debug("DeepSeek: Loading JSON data for processing")
            try:
                with open(input_data, 'r', encoding='utf-8') as file:
                    processed_data = json.load(file)
            except json.JSONDecodeError as e:
                logging.warning(f"DeepSeek: JSON decoding failed for file {input_data}: {e}. Treating as plain text.")
                # Fallback: read as plain text if JSON fails
                try:
                    with open(input_data, 'r', encoding='utf-8') as file:
                        processed_data = file.read()
                except Exception as read_e:
                     logging.error(f"DeepSeek: Failed to read file {input_data} as text: {read_e}")
                     return f"DeepSeek: Error reading input file {input_data}"
            except Exception as e:
                 logging.error(f"DeepSeek: Error processing file {input_data}: {e}")
                 return f"DeepSeek: Error processing input file {input_data}"
        else:
            logging.debug("DeepSeek: Using provided data directly (string/list/dict)")
            processed_data = input_data # Assume it's already loaded data (str, list, dict)

        # --- Text Extraction ---
        text_content = ""
        if isinstance(processed_data, dict) and 'summary' in processed_data:
            logging.debug("DeepSeek: Summary already exists in the input data.")
            return processed_data['summary']
        elif isinstance(processed_data, list):
            text_content = extract_text_from_segments(processed_data)
            logging.debug("DeepSeek: Extracted text from list of segments.")
        elif isinstance(processed_data, str):
            text_content = processed_data
            logging.debug("DeepSeek: Using string data directly.")
        elif isinstance(processed_data, dict):
             # Attempt to extract from common keys or serialize dict if it's not segments
             if 'segments' in processed_data and isinstance(processed_data['segments'], list):
                 text_content = extract_text_from_segments(processed_data['segments'])
                 logging.debug("DeepSeek: Extracted text from 'segments' key in dict.")
             elif 'text' in processed_data and isinstance(processed_data['text'], str):
                 text_content = processed_data['text']
                 logging.debug("DeepSeek: Extracted text from 'text' key in dict.")
             else:
                 logging.warning("DeepSeek: Input is dict but not segments/text format. Serializing.")
                 try: text_content = json.dumps(processed_data, indent=2)
                 except Exception: text_content = str(processed_data)
        else:
            logging.error(f"DeepSeek: Invalid input data format after processing: {type(processed_data)}")
            return "DeepSeek: Invalid input data format."

        if not text_content:
             logging.warning("DeepSeek: Text content is empty after processing input.")
             # Decide if this is an error or just proceed with empty content
             # return "DeepSeek: No text content found in input." # Optional error

        # --- Prepare Request ---
        headers = {
            'Authorization': f'Bearer {deepseek_api_key}',
            'Content-Type': 'application/json'
        }
        # Combine extracted text with the custom prompt argument
        deepseek_prompt = text_content
        if custom_prompt_arg:
            deepseek_prompt += f"\n\n{custom_prompt_arg}"
        logging.debug(f"DeepSeek: Final prompt (first 500 chars): {deepseek_prompt[:500]}...")

        payload = {
            "model": deepseek_model,
            "messages": [
                {"role": "system", "content": deepseek_system_message},
                {"role": "user", "content": deepseek_prompt}
            ],
            "stream": streaming_value,
            "temperature": temp_value,
            "top_p": top_p_value,
            "max_tokens": max_tokens_value # Add max_tokens if API supports it
        }

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        # Mount for both http and https, DeepSeek uses https
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        api_url = 'https://api.deepseek.com/chat/completions'

        if streaming_value:
            logging.debug(f"DeepSeek: Posting streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=api_timeout
            )
            response.raise_for_status() # Raise HTTP errors

            # --- Stream Processing Generator ---
            def stream_generator():
                collected_text = "" # Keep track if needed later
                try:
                    for line in response.iter_lines():
                        if line:
                            decoded_line = line.decode('utf-8').strip()
                            if decoded_line == '': continue
                            if decoded_line.startswith('data: '):
                                data_str = decoded_line[len('data: '):]
                                if data_str == '[DONE]': break
                                try:
                                    data_json = json.loads(data_str)
                                    # Check the structure DeepSeek actually sends
                                    if 'choices' in data_json and data_json['choices']:
                                         delta = data_json['choices'][0].get('delta', {})
                                         delta_content = delta.get('content', '')
                                         if delta_content: # Only yield non-empty content
                                             collected_text += delta_content
                                             yield delta_content
                                except json.JSONDecodeError:
                                    logging.error(f"DeepSeek: Error decoding JSON from line: {decoded_line}")
                                    continue # Skip malformed line
                                except (KeyError, IndexError) as e:
                                    logging.error(f"DeepSeek: Error parsing stream data structure: {e} in line: {decoded_line}")
                                    continue # Skip line with unexpected structure
                except Exception as stream_err:
                     logging.error(f"DeepSeek: Error during stream iteration: {stream_err}", exc_info=True)
                     # Optionally yield an error message chunk?
                     # yield f"[STREAM ERROR: {stream_err}]"
                finally:
                    logging.info(f"DeepSeek: Streaming finished. Total length: {len(collected_text)}")
                # yield collected_text # Remove if you only want chunks

            return stream_generator()
        else:
            logging.debug(f"DeepSeek: Posting non-streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=payload,
                timeout=api_timeout
            )
            logging.debug(f"DeepSeek: Response Status: {response.status_code}")

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"DeepSeek: Response Data: {json.dumps(response_data, indent=2)}") # Log full only if needed
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    message_content = response_data['choices'][0].get('message', {}).get('content')
                    if message_content:
                         chat_response = message_content.strip()
                         logging.info("DeepSeek: Chat request successful.")
                         # logging.debug(f"DeepSeek: Chat response: {chat_response[:200]}...")
                         return chat_response
                    else:
                         logging.warning("DeepSeek: 'content' field missing or empty in response message.")
                         return "DeepSeek: Response message content missing."
                else:
                    logging.warning("DeepSeek: 'choices' array missing or empty in response data.")
                    return "DeepSeek: Chat response format unexpected (no choices)."
            else:
                logging.error(f"DeepSeek: Chat request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"DeepSeek: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.Timeout:
        logging.error(f"DeepSeek: Request timed out after {api_timeout} seconds.")
        return f"DeepSeek: Request timed out."
    except requests.exceptions.RequestException as e:
        logging.error(f"DeepSeek: RequestException: {e}", exc_info=True)
        return f"DeepSeek: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        # This might happen if input_data is a file path to invalid JSON
        logging.error(f"DeepSeek: Error decoding JSON (input or response): {e}", exc_info=True)
        return f"DeepSeek: Error parsing JSON data."
    except ValueError as e: # Catch specific errors like invalid format
         logging.error(f"DeepSeek: Value error processing input: {e}", exc_info=True)
         return f"DeepSeek: Error processing input: {e}"
    except Exception as e:
        logging.error(f"DeepSeek: Unexpected error: {e}", exc_info=True)
        return f"DeepSeek: Unexpected error occurred: {e}"


def chat_with_mistral(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=None, topp=None, model=None):
    """Interacts with the Mistral API using settings from config."""
    provider_section_key = "mistralai" # Key used in [api_settings.*] in config.toml

    try:
        # --- Load Settings ---
        # API Key: Priority -> argument > env var (name from config)
        mistral_api_key = api_key # Prioritize argument
        if not mistral_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "MISTRAL_API_KEY")
            mistral_api_key = os.environ.get(api_key_env_var_name)
            if mistral_api_key:
                logging.info(f"Mistral: Using API key from environment variable {api_key_env_var_name}")

        if not mistral_api_key:
            logging.error("Mistral: API key not found in argument or environment variable.")
            return "Mistral: API Key Not Provided/Configured."
        logging.debug(f"Mistral: Using API Key: {mistral_api_key[:5]}...{mistral_api_key[-5:]}")

        # Model: Priority -> argument > config default
        mistral_model = model
        if not mistral_model:
            mistral_model = get_setting("api_settings", f"{provider_section_key}.model", "mistral-large-latest")
        logging.debug(f"Mistral: Using model: {mistral_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            if isinstance(streaming_cfg, str):
                streaming = streaming_cfg.lower() == "true"
            else:
                streaming = bool(streaming_cfg)
        else:
            streaming = bool(streaming)
        logging.debug(f"Mistral: Streaming mode: {streaming}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7)
            temp = _safe_cast(temp_cfg, float, 0.7)
        else:
            temp = _safe_cast(temp, float, 0.7)
        logging.debug(f"Mistral: Using temperature: {temp}")

        # Top_p (topp): Priority -> argument > config default
        # Mistral API uses 'top_p'. 'topp' is likely the UI variable name.
        if topp is None:
            topp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 1.0)
            top_p_value = _safe_cast(topp_cfg, float, 1.0)
        else:
            top_p_value = _safe_cast(topp, float, 1.0)
        logging.debug(f"Mistral: Using top_p: {top_p_value}")

        # Max Tokens: Load from config
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 4096)
        max_tokens = _safe_cast(max_tokens_cfg, int, 4096)
        logging.debug(f"Mistral: Using max_tokens: {max_tokens}")

        # System Message: Priority -> argument > fixed default
        if system_message is None:
            system_message = "You are a helpful AI assistant."
        logging.debug(f"Mistral: Using system message: {system_message[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 60)
        api_timeout = _safe_cast(timeout_cfg, int, 60)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, (int, float), 5) # backoff_factor can be float
        logging.debug(f"Mistral: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")

        # --- Input Data Handling ---
        # (Keep the existing logic to extract text from input_data)
        if isinstance(input_data, list):
            text = extract_text_from_segments(input_data) # Assuming this function exists and works
        elif isinstance(input_data, str):
            text = input_data
        else:
             logging.error(f"Mistral: Invalid input data format: {type(input_data)}")
             raise ValueError("Mistral: Invalid input data format")
        logging.debug("Mistral: Input data processed.")

        # --- Prepare Request ---
        headers = {
            'Authorization': f'Bearer {mistral_api_key}',
            'Content-Type': 'application/json',
            'Accept': 'application/json' # Explicitly accept JSON
        }
        mistral_prompt = f"{custom_prompt_arg}\n\n{text}" if custom_prompt_arg else text
        logging.debug(f"Mistral: Combined prompt (first 500 chars): {mistral_prompt[:500]}...")

        data = {
            "model": mistral_model,
            "messages": [
                # Mistral generally prefers user/assistant turns, system prompt can sometimes be implicitly handled
                # or added as the first message depending on the model. Check Mistral docs for best practice.
                # Option 1: System as first message (common for many models)
                 {"role": "system", "content": system_message},
                 {"role": "user", "content": mistral_prompt}
                # Option 2: Only user message if system prompt is basic
                # {"role": "user", "content": f"{system_message}\n\n{mistral_prompt}"} # Less standard
            ],
            "temperature": temp,
            "top_p": top_p_value, # Use the correct API parameter name
            "max_tokens": max_tokens,
            "stream": streaming,
            "safe_prompt": False # Or load from config if needed
            # Mistral doesn't typically use top_k or min_p in the main API
        }

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter) # Mount for https

        api_url = 'https://api.mistral.ai/v1/chat/completions' # Standard Mistral API URL

        if streaming:
            logging.debug(f"Mistral: Posting streaming request to {api_url}")
            headers['Accept'] = 'text/event-stream' # Necessary for Mistral streaming
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                stream=True,
                timeout=api_timeout
            )
            logging.debug(f"Mistral: Streaming Response Status: {response.status_code}")
            response.raise_for_status()

            # --- Stream Processing Generator ---
            def stream_generator() -> Generator[str, None, None]:
                buffer = ""
                for line_bytes in response.iter_lines():
                    if not line_bytes: continue
                    line = line_bytes.decode('utf-8').strip()

                    if line.startswith('data:'):
                        json_data_str = line[len('data:'):].strip()
                        if json_data_str == '[DONE]':
                            logging.debug("Mistral stream finished ([DONE] received).")
                            break
                        try:
                            data_chunk = json.loads(json_data_str)
                            if 'choices' in data_chunk and len(data_chunk['choices']) > 0:
                                delta = data_chunk['choices'][0].get('delta', {})
                                content_chunk = delta.get('content', '')
                                if content_chunk:
                                    yield content_chunk
                        except json.JSONDecodeError:
                            logging.error(f"Mistral: Error decoding stream JSON: {json_data_str}")
                            continue
                        except Exception as stream_err:
                             logging.error(f"Mistral: Error processing stream chunk: {stream_err} - Data: {json_data_str}", exc_info=True)
                             continue
                    else:
                        logging.debug(f"Mistral: Skipping non-data line in stream: {line}")

            return stream_generator()
        else:
            logging.debug(f"Mistral: Posting non-streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                timeout=api_timeout
            )
            logging.debug(f"Mistral: Response Status: {response.status_code}")

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"Mistral: Response Data: {response_data}")
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    chat_response = response_data['choices'][0]['message']['content'].strip()
                    logging.info("Mistral: Chat request successful.")
                    return chat_response
                else:
                    logging.warning("Mistral: Chat response not found in the response data.")
                    return "Mistral: Chat response format unexpected."
            else:
                logging.error(f"Mistral: Chat request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"Mistral: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e:
        logging.error(f"Mistral: RequestException: {e}", exc_info=True)
        return f"Mistral: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        logging.error(f"Mistral: Error decoding JSON response: {e}", exc_info=True)
        return f"Mistral: Error parsing API response."
    except Exception as e:
        logging.error(f"Mistral: Unexpected error: {e}", exc_info=True)
        return f"Mistral: Unexpected error occurred: {e}"


def chat_with_google(api_key, input_data, custom_prompt_arg, model=None, temp=None, system_message=None, streaming=None, topp=None, topk=None):
    """Interacts with the Google Gemini API via its OpenAI compatibility endpoint using settings from config."""
    provider_section_key = "google" # Key used in [api_settings.*] in config.toml

    try:
        # --- Load Settings ---
        # API Key: Priority -> argument > env var (name from config) > config file key (fallback)
        google_api_key = api_key # Prioritize argument
        if not google_api_key:
            api_key_env_var_name = get_setting("api_settings", f"{provider_section_key}.api_key_env_var", "GOOGLE_API_KEY")
            google_api_key = os.environ.get(api_key_env_var_name)
            if google_api_key:
                logging.info(f"Google: Using API key from environment variable {api_key_env_var_name}")
            # else: # Optional: Fallback to reading directly from config (less secure)
            #     google_api_key = get_setting("api_settings", f"{provider_section_key}.api_key")
            #     if google_api_key:
            #         logging.warning("Google: Using API key found directly in config file (less secure).")

        if not google_api_key:
            logging.error("Google: API key not found in argument, environment variable, or config.")
            return "Google: API Key Not Provided/Found/Configured."
        logging.debug(f"Google: Using API Key: {google_api_key[:5]}...{google_api_key[-5:]}")

        # Model: Priority -> argument > config default
        google_model = model
        if not google_model:
            google_model = get_setting("api_settings", f"{provider_section_key}.model", "gemini-1.5-pro-latest") # Default from config.toml
        logging.debug(f"Google: Using model: {google_model}")

        # Streaming: Priority -> argument > config default
        if streaming is None:
            streaming_cfg = get_setting("api_settings", f"{provider_section_key}.streaming", False)
            streaming = _safe_cast(streaming_cfg, bool, False)
        else:
             streaming = _safe_cast(streaming, bool, False) # Ensure boolean if passed as arg
        logging.debug(f"Google: Streaming mode: {streaming}")

        # Temperature: Priority -> argument > config default
        if temp is None:
            temp_cfg = get_setting("api_settings", f"{provider_section_key}.temperature", 0.7)
            temp_value = _safe_cast(temp_cfg, float, 0.7)
        else:
             temp_value = _safe_cast(temp, float, 0.7)
        logging.debug(f"Google: Using temperature: {temp_value}")

        # Top_p (topp): Priority -> argument > config default
        # Note: Google compatibility endpoint likely uses 'top_p'. 'topp' is UI variable name.
        if topp is None:
            topp_cfg = get_setting("api_settings", f"{provider_section_key}.top_p", 0.9)
            top_p_value = _safe_cast(topp_cfg, float, 0.9)
        else:
             top_p_value = _safe_cast(topp, float, 0.9)
        logging.debug(f"Google: Using top_p: {top_p_value}")

        # Top_k (topk): Priority -> argument > config default
        # Note: Google compatibility endpoint likely uses 'top_k'. 'topk' is UI variable name.
        if topk is None:
            topk_cfg = get_setting("api_settings", f"{provider_section_key}.top_k", 100) # Check default in config.toml
            top_k_value = _safe_cast(topk_cfg, int, 0) # Default to 0 if cast fails (often disables top_k)
        else:
             top_k_value = _safe_cast(topk, int, 0)
        logging.debug(f"Google: Using top_k: {top_k_value}")

        # Max Tokens: Load from config
        # Note: Google compatibility endpoint likely uses 'max_tokens'.
        max_tokens_cfg = get_setting("api_settings", f"{provider_section_key}.max_tokens", 8192) # Check default in config.toml
        max_tokens_value = _safe_cast(max_tokens_cfg, int, 8192)
        logging.debug(f"Google: Using max_tokens: {max_tokens_value}")

        # System Message: Priority -> argument > fixed default
        if system_message is None:
            system_message = "You are a helpful AI assistant." # Or load from config if needed
        logging.debug(f"Google: Using system message: {system_message[:100]}...")

        # Timeout, Retries, Delay: Load from config
        timeout_cfg = get_setting("api_settings", f"{provider_section_key}.timeout", 120)
        api_timeout = _safe_cast(timeout_cfg, int, 120)
        retries_cfg = get_setting("api_settings", f"{provider_section_key}.retries", 3)
        retry_count = _safe_cast(retries_cfg, int, 3)
        delay_cfg = get_setting("api_settings", f"{provider_section_key}.retry_delay", 5)
        retry_delay = _safe_cast(delay_cfg, int, 5) # Or float for backoff
        logging.debug(f"Google: Timeout={api_timeout}, Retries={retry_count}, Delay={retry_delay}")


        # --- Input Data Processing ---
        # (Keep your existing input data handling logic: file loading, json parsing, text extraction)
        logging.debug(f"Google: Raw input data type: {type(input_data)}")
        text = "" # Initialize text
        if isinstance(input_data, str):
            if input_data.strip().startswith('{'):
                 try: data = json.loads(input_data); text = json.dumps(data) # Or extract specific fields
                 except json.JSONDecodeError: logging.error("Google: Failed to parse JSON string, using raw string."); text = input_data
            elif os.path.isfile(input_data):
                try:
                     with open(input_data, 'r', encoding='utf-8') as f: data = json.load(f)
                     # Now extract text from the loaded JSON data (assuming segments)
                     if isinstance(data, dict) and 'segments' in data:
                         text = extract_text_from_segments(data['segments'])
                     elif isinstance(data, list):
                         text = extract_text_from_segments(data)
                     else:
                         logging.warning(f"Loaded JSON from {input_data} but couldn't find segments. Using JSON dump.")
                         text = json.dumps(data)
                except FileNotFoundError: logging.error(f"Input file not found: {input_data}"); return f"Google: Input file not found {input_data}"
                except json.JSONDecodeError: logging.error(f"Invalid JSON in file: {input_data}"); return f"Google: Invalid JSON file {input_data}"
                except Exception as e: logging.error(f"Error reading file {input_data}: {e}"); return f"Google: Error reading file {input_data}"
            else:
                 text = input_data # Assume it's plain text
        elif isinstance(input_data, list):
            text = extract_text_from_segments(input_data) # Assumes list of segments
        elif isinstance(input_data, dict):
             # Decide how to handle a dict input - maybe extract segments or dump?
             if 'segments' in input_data: text = extract_text_from_segments(input_data['segments'])
             elif 'text' in input_data: text = input_data['text']
             else: logging.warning("Received dict input without 'segments' or 'text', dumping."); text = json.dumps(input_data)
        else:
            logging.error(f"Google: Invalid input data format: {type(input_data)}")
            return f"Google: Invalid input data format {type(input_data)}"

        if not text:
            logging.warning("Google: Extracted text is empty.")
            # return "Google: Input data resulted in empty text." # Optionally return error

        logging.debug(f"Google: Extracted text (first 500 chars): {text[:500]}...")


        # --- Prepare Request ---
        headers = {
            # Google's OpenAI endpoint uses Bearer token (API Key)
            'Authorization': f'Bearer {google_api_key}',
            'Content-Type': 'application/json',
            # Accept header might not be strictly needed but doesn't hurt
            "Accept": "text/event-stream" if streaming else "application/json"
        }
        # Combine the extracted text with the custom prompt
        google_prompt = f"{text}\n\n{custom_prompt_arg}" if custom_prompt_arg else text
        logging.debug(f"Google: Combined prompt (first 500 chars): {google_prompt[:500]}...")

        # Construct payload using OpenAI compatible names
        data = {
            "model": google_model,
            "messages": [
                {"role": "system", "content": system_message},
                {"role": "user", "content": google_prompt}
            ],
            "temperature": temp_value,
            "top_p": top_p_value,
            # Only include top_k if it's > 0 (as 0 often means disabled)
            **({"top_k": top_k_value} if top_k_value > 0 else {}),
            "max_tokens": max_tokens_value,
            "stream": streaming
        }

        # --- Execute Request ---
        session = requests.Session()
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504], # Added 500
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        # Mount for HTTPS, as Google API uses it
        session.mount("https://", adapter)

        # Google OpenAI Compatibility Endpoint URL
        api_url = 'https://generativelanguage.googleapis.com/v1beta/openai/chat/completions'

        if streaming:
            logging.debug(f"Google: Posting streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                stream=True,
                timeout=api_timeout
            )
            logging.debug(f"Google: Raw Response Status: {response.status_code}")
            response.raise_for_status() # Raise HTTP errors

            # --- Stream Processing Generator ---
            def stream_generator():
                full_response_text = "" # Accumulate text for logging length
                try:
                    for line in response.iter_lines():
                        if line:
                            decoded_line = line.decode('utf-8').strip()
                            if decoded_line == '': continue
                            if decoded_line.startswith('data: '):
                                data_str = decoded_line[len('data: '):].strip()
                                if data_str == '[DONE]':
                                    logging.info(f"Google: Stream finished. Total length: {len(full_response_text)}")
                                    break
                                try:
                                    data_json = json.loads(data_str)
                                    # Parse OpenAI compatible streaming chunk
                                    chunk = data_json.get("choices", [{}])[0].get("delta", {}).get("content", "")
                                    if chunk: # Only yield non-empty chunks
                                         full_response_text += chunk
                                         yield chunk
                                except json.JSONDecodeError:
                                    logging.error(f"Google: Error decoding stream JSON: {decoded_line}")
                                except (KeyError, IndexError):
                                     logging.error(f"Google: Unexpected stream JSON structure: {data_json}")
                except Exception as stream_err:
                     logging.error(f"Google: Error during stream iteration: {stream_err}", exc_info=True)
                     # Yield an error message chunk if needed
                     yield f"\n[STREAM ERROR: {stream_err}]"
                finally:
                     response.close() # Ensure connection is closed
            return stream_generator() # Return the generator

        else: # Non-streaming
            logging.debug(f"Google: Posting non-streaming request to {api_url}")
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                timeout=api_timeout
            )
            logging.debug(f"Google: Response Status: {response.status_code}")

            if response.status_code == 200:
                response_data = response.json()
                # logging.debug(f"Google: Response Data: {response_data}") # Careful with logging
                # Parse OpenAI compatible response structure
                if 'choices' in response_data and len(response_data['choices']) > 0:
                    message_content = response_data['choices'][0].get('message', {}).get('content')
                    if message_content:
                        chat_response = message_content.strip()
                        logging.info("Google: Chat request successful.")
                        # logging.debug(f"Google: Chat response: {chat_response[:200]}...")
                        return chat_response
                    else:
                        logging.warning("Google: 'content' missing in response choices[0].message.")
                        return "Google: Response message content missing."
                else:
                    logging.warning("Google: 'choices' array missing or empty in response data.")
                    return "Google: Chat response format unexpected (no choices)."
            else:
                logging.error(f"Google: Chat request failed. Status: {response.status_code}, Body: {response.text[:500]}...")
                return f"Google: Failed request. Status: {response.status_code}"

    # --- Exception Handling ---
    except requests.exceptions.RequestException as e:
        logging.error(f"Google: RequestException: {e}", exc_info=True)
        return f"Google: Network or Request Error: {e}"
    except json.JSONDecodeError as e:
        # This might happen if input file is invalid JSON
        logging.error(f"Google: Error decoding JSON (input or response): {e}", exc_info=True)
        return f"Google: Error parsing JSON data."
    except Exception as e:
        logging.error(f"Google: Unexpected error: {e}", exc_info=True)
        return f"Google: Unexpected error occurred: {e}"

#
#
#######################################################################################################################